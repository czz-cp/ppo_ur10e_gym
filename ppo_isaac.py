"""
PPO (Proximal Policy Optimization) Implementation - Isaac Gymç‰ˆæœ¬

é’ˆå¯¹Isaac Gymä¼˜åŒ–çš„PPOå®ç°ï¼Œæ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒ
"""

# IMPORTANT: Isaac Gym must be imported before PyTorch
import numpy as np
import gym
from typing import Dict, Any, List, Tuple, Optional
import time
import os

from ur10e_env_isaac import UR10ePPOEnvIsaac
from ur10e_trajectory_env_isaac import UR10eTrajectoryEnvIsaac
from utils import (ValueNormalization, GAE, assert_same_device, check_tensor_devices,
                   get_tensor_device, ensure_device, get_forced_device)
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal


class ActorNetwork(nn.Module):
    """Actorç½‘ç»œ - è®ºæ–‡é£æ ¼ 3Ã—256 tanh MLPï¼Œé«˜æ–¯ç­–ç•¥ + tanh-squash + åŠ¨ä½œé›†æˆ"""

    def __init__(self, state_dim: int = 22, action_dim: int = 6, hidden_dim: int = 256):
        super().__init__()

        self.state_dim = state_dim
        self.action_dim = action_dim

        # å½’ä¸€åŒ–åŠ¨ä½œç©ºé—´ [-1, 1]^action_dim
        self.register_buffer(
            "action_limits_tensor",
            torch.ones(action_dim, dtype=torch.float32)
        )

        # ç‰¹å¾æå– MLPï¼š3 å±‚ Ã— 256, tanh æ¿€æ´»ï¼ˆå¯¹é½è®ºæ–‡ï¼‰
        self.feature_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
        )

        # ç‹¬ç«‹çš„ mean / log_std heads
        self.mean_head = nn.Linear(hidden_dim, action_dim)
        self.log_std_head = nn.Linear(hidden_dim, action_dim)

        self._init_actor_weights()

    def _init_actor_weights(self):
        """Orthogonal åˆå§‹åŒ– + å°è¾“å‡ºï¼Œé€‚é… tanh"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # tanh é€šå¸¸ç”¨ gain=1.0 å°±å¤Ÿäº†
                nn.init.orthogonal_(m.weight, gain=1.0)
                nn.init.constant_(m.bias, 0.0)

    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            state: [batch_size, state_dim]

        Returns:
            mean: [batch_size, action_dim]
            std:  [batch_size, action_dim]
        """

        x = self.feature_net(state)              # [B, 256]


        mean = self.mean_head(x)                 # [B, act_dim]
        log_std = self.log_std_head(x)           # [B, act_dim]

        # 1) å…ˆæŠŠéæœ‰é™å€¼å¤„ç†æ‰ï¼ˆä¸åˆ‡æ•´å›¾ï¼‰
        mean = torch.where(torch.isfinite(mean), mean, torch.zeros_like(mean))
        log_std = torch.where(torch.isfinite(log_std), log_std, torch.full_like(log_std, -2.0))

        # 2) é™åˆ¶ mean å¹…åº¦ï¼šé¿å… log_prob é‡Œå‡ºç° inf/-infï¼ˆè¶…é‡è¦ï¼‰
        #   ç”¨â€œè½¯é™åˆ¶â€æ¯” hard clamp æ›´å¹³æ»‘ï¼Œæ¢¯åº¦æ›´å¥åº·
        mean = 5.0 * torch.tanh(mean / 5.0)   # raw-space mean âˆˆ [-5, 5]

        # é˜²æ­¢ std å´©ï¼šé™åˆ¶ log_std èŒƒå›´
        log_std = torch.clamp(log_std, -4.0, 1.0)
        # å’Œä½ ç°åœ¨ä¸€è‡´ï¼Œç”¨ softplus æŠŠå®ƒå˜æˆæ­£æ•°
        #std = F.softplus(log_std)
        std = torch.exp(log_std)              # æ¯” softplus æ›´ç›´è§‚
        std = torch.clamp(std, 1e-3, 2.0)

        return mean, std

    def sample(self, state: torch.Tensor, use_delta_std: bool = True, delta_std: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ğŸ¯ Clip-only é‡‡æ ·ï¼ˆè®ºæ–‡ç‰ˆæœ¬ï¼‰

        æ ¸å¿ƒåŸåˆ™ï¼šlog_prob å¿…é¡»å¯¹åº”åŒä¸€ä¸ª"æœªæˆªæ–­çš„é«˜æ–¯å˜é‡"
        clip åªæ˜¯ç»™ env ç”¨çš„å®‰å…¨æ‰§è¡Œï¼Œä¸è¦æŠŠ clip ä¹‹åçš„å€¼å½“ä½œéšæœºå˜é‡å»ç®— log_prob

        Returns:
            action:   [-1, 1] å†…çš„å½’ä¸€åŒ–åŠ¨ä½œï¼ˆç»™ç¯å¢ƒæ‰§è¡Œç”¨ï¼‰
            log_prob: rawå˜é‡çš„log_probï¼ˆç”¨äºPPOè®¡ç®—ï¼‰
            raw:      æœªæˆªæ–­çš„åŸå§‹åŠ¨ä½œï¼ˆç”¨äºå­˜å‚¨å’Œé‡æ„ï¼‰
        """
        mean, std = self.forward(state)

        # ğŸ¯ ä½¿ç”¨å›ºå®šÎ´_stdï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if use_delta_std:
            std = torch.full_like(std, delta_std)

        dist = Normal(mean, std)
        raw = dist.rsample()              # ç”¨ rsample æ–¹ä¾¿åä¼ ï¼ˆPPOä¼šç”¨åˆ°ï¼‰
        action = torch.clamp(raw, -1.0, 1.0)  # ç»™ env æ‰§è¡Œç”¨
        log_prob = dist.log_prob(raw).sum(dim=-1)  # ğŸ¯ é‡è¦ï¼šå¯¹ raw ç®— log_prob

        return action, log_prob, raw
    def atanh(self, x: torch.Tensor) -> torch.Tensor:
        """æ•°å€¼å®‰å…¨çš„atanhå®ç°"""
        x = torch.clamp(x, -0.999, 0.999)
        return 0.5 * (torch.log1p(x) - torch.log1p(-x))

    def squashed_log_prob(self, dist: torch.distributions.Normal, actions: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—squashed Gaussiançš„log_prob

        Args:
            dist: torch.distributions.Normal, rawç©ºé—´çš„é«˜æ–¯åˆ†å¸ƒ
            actions: torch.Tensor, [-1,1]èŒƒå›´çš„squashedåŠ¨ä½œ

        Returns:
            log_prob: torch.Tensor, è€ƒè™‘tanhå˜æ¢çš„logæ¦‚ç‡
        """
        # å°†squashedåŠ¨ä½œè¿˜åŸåˆ°rawç©ºé—´
        raw = self.atanh(actions)
        # è®¡ç®—rawç©ºé—´çš„log_prob
        logp = dist.log_prob(raw).sum(dim=-1)
        # å‡å»tanhå˜æ¢çš„Jacobianå¯¹æ•°è¡Œåˆ—å¼: log|det(âˆ‚tanh/âˆ‚raw)|
        # âˆ‚tanh/âˆ‚raw = 1 - tanhÂ²(raw) = 1 - actionsÂ²
        # å› ä¸º1 - actionsÂ² > 0ï¼ˆ|actions| < 1ï¼‰ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥ç”¨log
        jacobian_log = torch.log(1 - actions * actions + 1e-6).sum(dim=-1)
        logp -= jacobian_log
        return logp

    def get_dist(self, states: torch.Tensor, fixed_std: float = None) -> torch.distributions.Normal:
        """
        è·å–ç­–ç•¥åˆ†å¸ƒï¼Œæ”¯æŒå›ºå®šæ ‡å‡†å·®

        Args:
            states: [batch_size, state_dim]
            fixed_std: float, å¦‚æœæä¾›åˆ™ä½¿ç”¨å›ºå®šstdï¼Œå¦åˆ™ä½¿ç”¨ç½‘ç»œstd

        Returns:
            dist: torch.distributions.Normal
        """
        mean, std = self.forward(states)
        if fixed_std is not None:
            std = torch.ones_like(std) * fixed_std
        return torch.distributions.Normal(mean, std)

    def compute_aew_ensemble_size(self, current_episode: int, max_episodes: int,
                                alpha: float = 5.0, beta: float = 8.0, lambda_max: float = None) -> int:
        """
        è®¡ç®—AEWï¼ˆWeibull Action Ensemblesï¼‰çš„é‡‡æ ·æ¬¡æ•°

        æ ¹æ®è®ºæ–‡ï¼ši ~ clip(Weibull(k, Î»), 1, Î»)
        å…¶ä¸­ k = 1 + Î± * episode / episode_max, Î» = 1 + Î² * episode / episode_max

        Args:
            current_episode: å½“å‰è®­ç»ƒepisode
            max_episodes: æœ€å¤§è®­ç»ƒepisodeæ•°
            alpha: Weibullå½¢çŠ¶å‚æ•°å¢é•¿ç³»æ•°
            beta: Weibullå°ºåº¦å‚æ•°å¢é•¿ç³»æ•°

        Returns:
            ensemble_size: é‡‡æ ·æ¬¡æ•° i
        """
        progress = current_episode / max(max_episodes, 1)  # é˜²æ­¢é™¤é›¶

        # è®¡ç®—Weibullåˆ†å¸ƒå‚æ•°
        k = 1.0 + alpha * progress  # å½¢çŠ¶å‚æ•°
        lam = 1.0 + beta * progress  # å°ºåº¦å‚æ•°

        # ä»Weibullåˆ†å¸ƒé‡‡æ ·
        if torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')

        # Weibullé‡‡æ ·ï¼šu ~ Uniform(0,1), x = Î» * (-log(u))^(1/k)
        u = torch.rand(1, device=device)
        weibull_sample = lam * (-torch.log(u)).pow(1.0 / k)

        # ä½¿ç”¨ä½ å»ºè®®çš„æ–¹æ³•ï¼šmax(1, round(i))ï¼Œç„¶åclamp(1, max(lam, lambda_max))
        max_ensemble = lambda_max if lambda_max is not None else lam
        ensemble_size = int(torch.clamp(weibull_sample.round(), 1, max_ensemble).item())

        return ensemble_size
    
    def sample_clip(self, state, delta_std: float):
        mean, _ = self.forward(state)
        std = torch.full_like(mean, delta_std)
        dist = Normal(mean, std)

        raw = dist.sample()                       # raw action
        exec_action = torch.clamp(raw, -1.0, 1.0) # clip only for env
        log_prob = dist.log_prob(raw).sum(-1)     # IMPORTANT: prob of raw
        return exec_action, log_prob, raw


    def sample_with_ensemble_clip(self, state, ensemble_size: int, delta_std: float):
        mean, _ = self.forward(state)
        std = torch.full_like(mean, delta_std)

        B, A = mean.shape
        i = ensemble_size
        raw_samples = Normal(
            mean.unsqueeze(1).expand(B, i, A),
            std.unsqueeze(1).expand(B, i, A)
        ).sample()                                # [B,i,A]

        raw_mean = raw_samples.mean(dim=1)        # [B,A]
        exec_action = torch.clamp(raw_mean, -1.0, 1.0)

        eff_std = std / torch.sqrt(torch.tensor(float(i), device=std.device))
        eff_dist = Normal(mean, eff_std)
        log_prob = eff_dist.log_prob(raw_mean).sum(-1)

        return exec_action, log_prob, raw_mean


    def log_prob_ensemble_rawmean(self, state, raw_mean, ensemble_size: int, delta_std: float):
        mean, _ = self.forward(state)
        std = torch.full_like(mean, delta_std)
        eff_std = std / torch.sqrt(torch.tensor(float(ensemble_size), device=std.device))
        eff_dist = Normal(mean, eff_std)
        return eff_dist.log_prob(raw_mean).sum(-1)



class CriticNetwork(nn.Module):
    """Criticç½‘ç»œ - è®ºæ–‡é£æ ¼ 3Ã—256 tanh MLP"""

    def __init__(self, state_dim: int = 22, hidden_dim: int = 256):
        super().__init__()

        self.value_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1),
        )

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight.data, gain=1.0)
            nn.init.constant_(module.bias.data, 0.0)

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """
        Args:
            state: [batch_size, state_dim]

        Returns:
            value: [batch_size, 1]
        """
        return self.value_net(state)


class PPOIsaac:
    """
    PPOç®—æ³•å®ç° - Isaac Gymç‰ˆæœ¬

    ä¸“é—¨é’ˆå¯¹Isaac Gymå¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒä¼˜åŒ–
    é›†æˆä»·å€¼å½’ä¸€åŒ–ã€GAEã€æ¢¯åº¦è£å‰ªç­‰æŠ€æœ¯
    """

    def __init__(self,
                 env: UR10eTrajectoryEnvIsaac,
                 config: Dict[str, Any]):
        """
        åˆå§‹åŒ–PPOè®­ç»ƒå™¨

        Args:
            env: Isaac Gymç¯å¢ƒ
            config: è®­ç»ƒé…ç½®
        """
        self.env = env
        self.config = config

        # ğŸ¯ [SERVER FIX] ä½¿ç”¨ç¯å¢ƒè®¾å¤‡ï¼ˆå‚è€ƒisaac_gym_manipulatorå®ç°æ¨¡å¼ï¼‰
        self.device = env.device
        print(f"ğŸ”’ [ENV DEVICE] PPOä½¿ç”¨ç¯å¢ƒè®¾å¤‡: {self.device}")

        # ğŸš¨ [SERVER SAFETY] éªŒè¯è®¾å¤‡ä¸€è‡´æ€§
        forced_device = get_forced_device()
        if str(self.device) != str(forced_device):
            print(f"âš ï¸ [DEVICE MISMATCH] ç¯å¢ƒè®¾å¤‡{self.device} != å¼ºåˆ¶è®¾å¤‡{forced_device}")
            print(f"   å¼ºåˆ¶ä½¿ç”¨ç¯å¢ƒè®¾å¤‡: {self.device}")
            # åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­ï¼Œå¼ºåˆ¶ç¯å¢ƒè®¾å¤‡ä¸ºcuda:0
            if torch.cuda.is_available() and torch.cuda.device_count() > 1:
                os.environ['CUDA_VISIBLE_DEVICES'] = '0'

        # ç¯å¢ƒå‚æ•°
        self.num_envs = env.get_num_envs()
        self.state_dim = env.get_num_obs()
        self.action_dim = env.get_num_actions()

        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = ActorNetwork(self.state_dim, self.action_dim).to(self.device)
        self.critic = CriticNetwork(self.state_dim).to(self.device)

        # ç¡®ä¿ç½‘ç»œå‚æ•°åœ¨æ­£ç¡®è®¾å¤‡ä¸Š (ä¿®å¤è®¾å¤‡ä¸€è‡´æ€§)
        assert next(self.actor.parameters()).device == self.device, "Actor not on correct device"
        assert next(self.critic.parameters()).device == self.device, "Critic not on correct device"

        # æ˜¾å¼è®¾ç½®ç½‘ç»œå‚æ•°requires_grad=True
        for param in self.actor.parameters():
            param.requires_grad = True
        for param in self.critic.parameters():
            param.requires_grad = True

        # éªŒè¯æ¢¯åº¦è®¾ç½®
        assert all(p.requires_grad for p in self.actor.parameters()), "Actorå‚æ•°æœªè®¾ç½®requires_grad"
        assert all(p.requires_grad for p in self.critic.parameters()), "Criticå‚æ•°æœªè®¾ç½®requires_grad"

        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.AdamW(
            self.actor.parameters(),
            lr=float(config['ppo']['lr_actor'])
        )
        self.critic_optimizer = optim.AdamW(
            self.critic.parameters(),
            lr=float(config['ppo']['lr_critic'])
        )

        # ä»·å€¼å½’ä¸€åŒ–å™¨
        self.value_norm = ValueNormalization(
            beta=0.995,
            epsilon=1e-5,
            clip_range=10.0
        ).to(self.device)

        # ğŸ¯ åŠ¨ä½œé›†æˆï¼ˆAction Ensembles, AEï¼‰é…ç½®
        ae_config = config.get('ae', {})
        self.ae_enabled = ae_config.get('enabled', False)
        self.ae_alpha = float(ae_config.get('alpha', 5.0))
        self.ae_beta = float(ae_config.get('beta', 8.0))
        self.ae_lambda_max = int(ae_config.get('lambda_max', 10))  # æœ€å¤§é›†æˆå¤§å°
        self.ae_delta_std = float(ae_config.get('delta_std', 0.1))
        self.current_ensemble_size = 1  # é»˜è®¤é‡‡æ ·æ¬¡æ•°

        # ğŸ¯ ç­–ç•¥åé¦ˆï¼ˆPolicy Feedback, PFï¼‰é…ç½®
        pf_config = config.get('pf', {})
        self.pf_enabled = pf_config.get('enabled', False)
        self.pf_eta_min = float(pf_config.get('eta_min', 0.6))
        self.pf_eta_max = float(pf_config.get('eta_max', 0.99))

        # GAEè®¡ç®—å™¨ï¼ˆæ”¯æŒç­–ç•¥åé¦ˆï¼‰- å¿…é¡»åœ¨pf_configå®šä¹‰ä¹‹å
        self.gae = GAE(
            gamma=float(config['ppo']['gamma']),
            lam=float(config['ppo']['lam']),
            device=self.device,
            use_adaptive_gamma=self.pf_enabled,
            eta_min=self.pf_eta_min,
            eta_max=self.pf_eta_max
        )

        # è®­ç»ƒå‚æ•° - ç¡®ä¿ç±»å‹è½¬æ¢
        self.clip_eps = float(config['ppo']['clip_eps'])
        self.entropy_coef = float(config['ppo']['entropy_coef'])
        self.value_coef = float(config['ppo']['value_coef'])
        self.max_grad_norm = float(config['ppo']['max_grad_norm'])

        # ç¼“å†²åŒºå‚æ•° - ç¡®ä¿æ•´æ•°ç±»å‹
        self.rollout_length = int(config['train']['rollout_length'])
        self.batch_size = int(config['train']['batch_size'])
        self.num_updates = int(config['train']['num_updates'])
        self.num_episodes = int(config['train']['num_episodes'])

        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_count = 0
        self.total_steps = 0
        self.best_performance = -float('inf')

        print(f"ğŸ¤– Isaac Gym PPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   å¹¶è¡Œç¯å¢ƒæ•°: {self.num_envs}")
        print(f"   çŠ¶æ€ç»´åº¦: {self.state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"   è®¾å¤‡: {self.device}")

        # ğŸ¯ æ˜¾ç¤ºAEï¿½ï¿½ï¿½PFçŠ¶æ€
        print(f"   ğŸ¯ åŠ¨ä½œé›†æˆ(AE): {'å¯ç”¨' if self.ae_enabled else 'ç¦ç”¨'}")
        if self.ae_enabled:
            print(f"      Alpha: {self.ae_alpha}, Beta: {self.ae_beta}, Lambda_max: {self.ae_lambda_max}, Delta_std: {self.ae_delta_std}")
        print(f"   ğŸ¯ ç­–ç•¥åé¦ˆ(PF): {'å¯ç”¨' if self.pf_enabled else 'ç¦ç”¨'}")
        if self.pf_enabled:
            print(f"      EtaèŒƒå›´: [{self.pf_eta_min}, {self.pf_eta_max}]")

        # æ¢¯åº¦è®¡ç®—æµ‹è¯•
        if not self._test_gradient_flow():
            print("âŒ æ¢¯åº¦è®¡ç®—æµ‹è¯•å¤±è´¥")
            raise RuntimeError("æ¢¯åº¦è®¡ç®—æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå®ç°")

    def update_ensemble_size(self):
        """æ ¹æ®å½“å‰è®­ç»ƒè¿›åº¦æ›´æ–°AEé‡‡æ ·æ¬¡æ•°"""
        if self.ae_enabled:
            self.current_ensemble_size = self.actor.compute_aew_ensemble_size(
                current_episode=self.episode_count,
                max_episodes=self.num_episodes,
                alpha=self.ae_alpha,
                beta=self.ae_beta,
                lambda_max=self.ae_lambda_max
            )

    def _test_gradient_flow(self):
        """æµ‹è¯•æ¢¯åº¦è®¡ç®—æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_states = torch.randn(2, self.state_dim, device=self.device, requires_grad=True)
            test_actions = torch.randn(2, self.action_dim, device=self.device)
            test_old_log_probs = torch.randn(2, device=self.device)
            test_advantages = torch.randn(2, device=self.device)

            # æµ‹è¯•actoræ¢¯åº¦
            new_means, new_stds = self.actor(test_states)
            dist = Normal(new_means, new_stds)
            new_log_probs = dist.log_prob(test_actions).sum(dim=-1)
            ratio = torch.exp(new_log_probs - test_old_log_probs)
            actor_loss = -(ratio * test_advantages).mean()

            assert actor_loss.requires_grad, "ActoræŸå¤±æ²¡æœ‰æ¢¯åº¦"
            actor_loss.backward()

            # æ£€æŸ¥actorå‚æ•°æ¢¯åº¦
            actor_has_grad = any(p.grad is not None for p in self.actor.parameters())
            assert actor_has_grad, "Actorå‚æ•°æ²¡æœ‰æ¢¯åº¦"

            # æµ‹è¯•criticæ¢¯åº¦
            test_values = self.critic(test_states).squeeze(-1)
            test_returns = torch.randn(2, device=self.device)
            critic_loss = F.mse_loss(test_values, test_returns)

            assert critic_loss.requires_grad, "CriticæŸå¤±æ²¡æœ‰æ¢¯åº¦"
            critic_loss.backward()

            # æ£€æŸ¥criticå‚æ•°æ¢¯åº¦
            critic_has_grad = any(p.grad is not None for p in self.critic.parameters())
            assert critic_has_grad, "Criticå‚æ•°æ²¡æœ‰æ¢¯åº¦"

            # æ¸…ç†æ¢¯åº¦
            self.actor_optimizer.zero_grad()
            self.critic_optimizer.zero_grad()

            print("âœ… æ¢¯åº¦è®¡ç®—æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            print(f"âŒ æ¢¯åº¦è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            return False

    def collect_rollouts(self) -> Dict[str, torch.Tensor]:
        """
        æ”¶é›†ç»éªŒå›æ”¾æ•°æ®

        Returns:
            rollouts: æ”¶é›†çš„æ•°æ®å­—å…¸
        """
        # é‡ç½®ç¯å¢ƒ
        reset_result = self.env.reset()
        # Handle both single obs and (obs, info) return formats
        if isinstance(reset_result, tuple):
            states, info = reset_result
            # Store info for potential debugging (suppress unused warning)
            _ = info
        else:
            states = reset_result

        # ç¡®ä¿çŠ¶æ€åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        states = ensure_device(states, self.device)

        # åˆå§‹åŒ–ç¼“å†²åŒº
        rollouts = {
            'states': [],
            'actions': [],
            'log_probs': [],
            'raw_means': [],  # ğŸ¯ é…å¥—è¦æ±‚ï¼šå­˜å‚¨pre-clipçš„raw_means
            'values': [],
            'rewards': [],
            'dones': [],
            'next_states': []
        }

        episode_rewards = np.zeros(self.num_envs)
        episode_lengths = np.zeros(self.num_envs)

        # ğŸ¯ A) ä¿®å¤ï¼šä¸€ä¸ªrolloutåªç”¨ä¸€ä¸ªå›ºå®šçš„ensemble size
        if self.ae_enabled:
            self.update_ensemble_size()
            rollout_ensemble_size = int(self.current_ensemble_size)
        else:
            rollout_ensemble_size = 1

        for step in range(self.rollout_length):
            # ç¡®ä¿statesæ˜¯2Då¼ é‡ [num_envs, state_dim]
            if states.ndim == 1:
                states = states.unsqueeze(0)  # [state_dim] -> [1, state_dim]
            # è®°å½•å½“å‰çŠ¶æ€
            rollouts['states'].append(states.clone())

            # é‡‡æ ·åŠ¨ä½œ (æ•°æ®æ”¶é›†æ—¶ä½¿ç”¨no_gradï¼Œä½†çŠ¶æ€éœ€è¦æ¢¯åº¦)
            states_for_sampling = states.detach()
            with torch.no_grad():
                # ğŸ¯ ä½¿ç”¨å®Œå…¨è´´è®ºæ–‡çš„clip-onlyæ–¹æ³•
                if self.ae_enabled:
                    # ğŸ¯ ä½¿ç”¨è®ºæ–‡ç‰ˆAEï¼šå…ˆå¹³å‡rawï¼Œå†clipï¼›log_probç”¨"å‡å€¼åˆ†å¸ƒ"
                    actions, log_probs, raw_means = self.actor.sample_with_ensemble_clip(
                        states_for_sampling,
                        ensemble_size=rollout_ensemble_size,
                        delta_std=self.ae_delta_std
                    )
                else:
                    # ğŸ¯ æ ‡å‡†PPOé‡‡æ · - å®Œå…¨clip-onlyç‰ˆæœ¬
                    actions, log_probs, raw_actions = self.actor.sample_clip(
                        states_for_sampling,
                        delta_std=self.ae_delta_std if self.ae_enabled else self.ae_delta_std
                    )
                    raw_means = raw_actions  # æ ‡å‡†æ¨¡å¼ä¸‹ï¼Œraw_meanså°±æ˜¯raw_actions

                values = self.critic(states_for_sampling)

            # è°ƒè¯•ä¿¡æ¯ (æ¯64æ­¥æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦)
            if step % 50 == 0 and hasattr(self.env, 'episode_steps'):
                avg_episode_steps = self.env.episode_steps.mean().item()
                max_episode_steps = self.env.episode_steps.max().item()
                print(f"ğŸ“ˆ Step {step:3d}: å¹³å‡episodeæ­¥æ•°: {avg_episode_steps:.1f}, æœ€å¤§: {max_episode_steps}")

            # æ‰§è¡ŒåŠ¨ä½œ (Gymnasiumæ ¼å¼è¿”å›5ä¸ªå€¼)
            step_result = self.env.step(actions)
            if len(step_result) == 5:
                # Gymnasiumæ ¼å¼: (obs, reward, terminated, truncated, info)
                next_states, rewards, terminated, truncated, infos = step_result
                dones = np.logical_or(terminated, truncated)  # åˆå¹¶terminatedå’Œtruncated
            elif len(step_result) == 4:
                # æ—§æ ¼å¼: (obs, reward, done, info)
                next_states, rewards, dones, infos = step_result
            else:
                raise ValueError(f"ç¯å¢ƒstepè¿”å›äº†{len(step_result)}ä¸ªå€¼ï¼ŒæœŸæœ›4æˆ–5ä¸ª")

            # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            next_states = ensure_device(next_states, self.device)
            rewards = ensure_device(rewards, self.device)
            dones = ensure_device(dones, self.device)

            # å¤„ç†numpyæ•°ç»„è½¬æ¢ä¸ºå¼ é‡
            if isinstance(dones, np.ndarray):
                dones = torch.tensor(dones, dtype=torch.bool, device=self.device)
            elif not isinstance(dones, torch.Tensor):
                dones = torch.tensor([dones], dtype=torch.bool, device=self.device)

            # ç¡®ä¿donesæ˜¯æ­£ç¡®çš„å½¢çŠ¶
            if dones.dim() == 0:
                dones = dones.unsqueeze(0)  # [ ] -> [1]
            elif dones.dim() > 1:
                dones = dones.flatten()  # -> [num_envs]

            # åŒæ ·å¤„ç†rewards
            if isinstance(rewards, (float, int, np.float32, np.float64, np.int32, np.int64)):
                rewards = torch.tensor([rewards], dtype=torch.float32, device=self.device)
            elif isinstance(rewards, np.ndarray):
                rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)
            elif not isinstance(rewards, torch.Tensor):
                rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)

            # ç¡®ä¿rewardsæ˜¯æ­£ç¡®çš„å½¢çŠ¶
            if rewards.dim() == 0:
                rewards = rewards.unsqueeze(0)  # [ ] -> [1]
            elif rewards.dim() > 1:
                rewards = rewards.flatten()  # -> [num_envs]

            # è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥ (ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜)
            try:
                assert_same_device(states, actions, next_states, rewards, dones, device=self.device)
            except AssertionError as e:
                print(f"âŒ è®¾å¤‡ä¸åŒ¹é…é”™è¯¯: {e}")
                print(f"   states: {states.device}")
                print(f"   actions: {actions.device}")
                print(f"   next_states: {next_states.device}")
                print(f"   rewards: {rewards.device}")
                print(f"   dones: {dones.device}")
                print(f"   æœŸæœ›è®¾å¤‡: {self.device}")
                raise

            # è®°å½•æ•°æ®
            rollouts['actions'].append(actions.clone())
            rollouts['log_probs'].append(log_probs.clone())
            rollouts['raw_means'].append(raw_means.clone())  # ğŸ¯ å­˜å‚¨pre-clipçš„raw_means
            rollouts['values'].append(values.squeeze(-1).clone())
            rollouts['rewards'].append(rewards.clone())
            rollouts['dones'].append(dones.clone())
            rollouts['next_states'].append(next_states.clone())

            # ç»Ÿè®¡ä¿¡æ¯ (ä¿®å¤è®¾å¤‡ä¸åŒ¹é…)
            rewards_device = rewards.device
            episode_rewards += rewards.detach().cpu().numpy()
            episode_lengths += 1

            # è®¾å¤‡ä¸€è‡´æ€§è°ƒè¯•ä¿¡æ¯
            if rewards_device != self.device:
                print(f"âš ï¸ è®¾å¤‡ä¸åŒ¹é…: rewardsåœ¨{rewards_device}, æœŸæœ›åœ¨{self.device}")

            # å¤„ç†å®Œæˆçš„å›åˆ
            for i in range(self.num_envs):
                if i < dones.shape[0] and dones[i]:
                    self.episode_count += 1
                    self.total_steps += episode_lengths[i]

                    print(f"ğŸ¯ Episodeå®Œæˆ! ç¯å¢ƒ{i}, å¥–åŠ±: {episode_rewards[i]:.4f}, æ­¥æ•°: {episode_lengths[i]}")

                    # æ›´æ–°æœ€ä½³æ€§èƒ½
                    if episode_rewards[i] > self.best_performance:
                        self.best_performance = episode_rewards[i]
                        print(f"ğŸ† æ–°æœ€ä½³æ€§èƒ½! {self.best_performance:.4f}")

                    # é‡ç½®ç»Ÿè®¡
                    episode_rewards[i] = 0
                    episode_lengths[i] = 0

            states = next_states

        # ğŸ¯ æ·»åŠ ensemble sizeåˆ°rolloutsä¸­ï¼ˆç”¨äºupdateé˜¶æ®µä¸€è‡´æ€§ï¼‰
        # âœ… ç¡®ä¿å­˜å‚¨ä¸ºtensorï¼Œæ–¹ä¾¿åç»­è®¾å¤‡ç®¡ç†
        rollouts['ensemble_size'] = torch.tensor(rollout_ensemble_size, device=self.device)

        # è½¬æ¢ä¸ºå¼ é‡
        for key in rollouts:
            if key not in ['next_states', 'ensemble_size']:
                rollouts[key] = torch.stack(rollouts[key], dim=0)  # [rollout_length, num_envs]

        return rollouts

    def update_policy(self, rollouts: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        æ›´æ–°ç­–ç•¥ç½‘ç»œ

        Args:
            rollouts: ç»éªŒå›æ”¾æ•°æ®

        Returns:
            metrics: è®­ç»ƒæŒ‡æ ‡
        """
        # ğŸ¯ [SERVER FIX] ç¡®ä¿æ‰€æœ‰rolloutæ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        for key, tensor in rollouts.items():
            if isinstance(tensor, torch.Tensor) and tensor.device != self.device:
                print(f"âš ï¸ [DEVICE FIX] {key}ä»{tensor.device}ç§»åŠ¨åˆ°{self.device}")
                rollouts[key] = tensor.to(self.device)

        # å‡†å¤‡æ•°æ®
        states = rollouts['states'].view(-1, self.state_dim)  # [T*N, state_dim]
        actions = rollouts['actions'].view(-1, self.action_dim)  # [T*N, action_dim]
        old_log_probs = rollouts['log_probs'].view(-1)  # [T*N]
        raw_means = rollouts['raw_means'].view(-1, self.action_dim)  # ğŸ¯ é…å¥—ï¼šå–å‡ºå­˜å‚¨çš„raw_means

        # ğŸ”§ ä¿®å¤ï¼šè®¡ç®—ä»·å€¼å’Œä¼˜åŠ¿ - GAEéœ€è¦åŸå§‹å°ºåº¦çš„values
        values_raw = rollouts['values'].view(self.rollout_length, self.num_envs)  # [T, N] - åŸå§‹ç½‘ç»œè¾“å‡º
        # ğŸ”§ ä¿è¯value_normå­˜åœ¨æ—¶æ‰denormalizeï¼ˆç¨³å¥æ€§æ”¹è¿›ï¼‰
        if self.value_norm is not None:
            values = self.value_norm.denormalize(values_raw)  # åå½’ä¸€åŒ–åˆ°åŸå§‹å¥–åŠ±å°ºåº¦ç”¨äºGAE
        else:
            values = values_raw
        rewards = rollouts['rewards'].view(self.rollout_length, self.num_envs)  # [T, N]
        dones = rollouts['dones'].view(self.rollout_length, self.num_envs)  # [T, N]

        # ğŸ” [CRITICAL CHECK] éªŒè¯è®¾å¤‡ä¸€è‡´æ€§ï¼ˆé¢„é˜²ç¬¬500æ­¥é”™è¯¯ï¼‰
        try:
            assert_same_device(states, actions, old_log_probs, values, rewards, dones, device=self.device)
            print(f"âœ… [DEVICE OK] æ‰€æœ‰rolloutæ•°æ®åœ¨{self.device}ä¸Š")
        except AssertionError as e:
            print(f"âŒ [DEVICE ERROR] {e}")
            # å¼ºåˆ¶ä¿®å¤
            states = states.to(self.device)
            actions = actions.to(self.device)
            old_log_probs = old_log_probs.to(self.device)
            values = values.to(self.device)
            rewards = rewards.to(self.device)
            dones = dones.to(self.device)

        # è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼
        with torch.no_grad():
            last_next_state = rollouts['next_states'][-1].to(self.device)
            next_values_raw = self.critic(last_next_state).squeeze(-1)  # [N] - åŸå§‹ç½‘ç»œè¾“å‡º
            # ğŸ”§ ä¿è¯value_normå­˜åœ¨æ—¶æ‰denormalizeï¼ˆç¨³å¥æ€§æ”¹è¿›ï¼‰
            if self.value_norm is not None:
                next_values = self.value_norm.denormalize(next_values_raw)  # åå½’ä¸€åŒ–åˆ°åŸå§‹å¥–åŠ±å°ºåº¦
            else:
                next_values = next_values_raw
            # ä¿®å¤ï¼šä¸ºGAEå‡½æ•°åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„next_values [T, N]
            next_values_expanded = next_values.unsqueeze(0).expand(self.rollout_length, -1)  # [T, N]

        # è®¡ç®—GAEä¼˜åŠ¿å’Œå›æŠ¥ï¼ˆæ”¯æŒç­–ç•¥åé¦ˆï¼‰
        if self.pf_enabled:
            # âœ… ç­–ç•¥åé¦ˆå…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨rolloutæ—¶å­˜ï¿½ï¿½çš„log_probsè®¡ç®—action_probs
            # old_log_probså·²ç»åœ¨rollouté˜¶æ®µä½¿ç”¨ä¸AE/atanhå¯¹é½çš„å®Œæ•´è®¡ç®—æµç¨‹
            # old_log_probs: [T*N] -> [T, N]
            old_lp = rollouts['log_probs'].view(self.rollout_length, self.num_envs)

            # âœ… PFæ ¸å¿ƒï¼šç›´æ¥ä½¿ç”¨rolloutå­˜å‚¨çš„log_probsï¼Œé¿å…é‡æ–°è®¡ç®—å¯¼è‡´çš„ä¸å¯¹é½
            # è®ºæ–‡å®šä¹‰: Î³ = clip(Ï€(s,a), Î·, 1)ï¼Œå…¶ä¸­Ï€(s,a)æ˜¯rolloutæ—¶çš„ç­–ç•¥æ¦‚ç‡
            # è¿ç»­åŠ¨ä½œå¯†åº¦å¯èƒ½>1ï¼Œå› æ­¤å…ˆç”¨clampç¡®ä¿Ï€(s,a) <= 1
            action_probs = torch.exp(torch.clamp(old_lp, max=0.0))

            advantages, returns = self.gae(rewards, dones, values, next_values_expanded, action_probs)
        else:
            # æ ‡å‡†GAE
            advantages, returns = self.gae(rewards, dones, values, next_values_expanded)

        # å±•å¹³
        advantages = advantages.view(-1).float()  # [T*N]
        returns = returns.view(-1).float()  # [T*N]

        # å½’ä¸€åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # æ›´æ–°ä»·å€¼å½’ä¸€åŒ–å™¨
        self.value_norm.update(returns)

        # å¤šæ¬¡æ›´æ–°
        total_actor_loss = 0
        total_critic_loss = 0
        total_entropy = 0
        kl_early_stops = 0  # è®°å½•KL early-stopæ¬¡æ•°
        stop_update = False  # âœ… KL early-stopæ ‡å¿—ï¼ˆå¤–å±‚è·³å‡ºç”¨ï¼‰

        for update_epoch in range(self.num_updates):
            # éšæœºé‡‡æ ·æ‰¹æ¬¡
            indices = torch.randperm(states.shape[0], device=self.device)

            for start in range(0, states.shape[0], self.batch_size):
                end = min(start + self.batch_size, states.shape[0])
                batch_indices = indices[start:end]

                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_returns = returns[batch_indices]
                batch_raw_means = raw_means[batch_indices]  # ğŸ¯ é…å¥—ï¼šå–å‡ºå¯¹åº”çš„raw_means

                # ğŸ¯ B) ä¿®å¤ï¼šä½¿ç”¨rolloutä¸­å­˜å‚¨çš„ensemble sizeç¡®ä¿ä¸€è‡´æ€§
                if self.ae_enabled:
                    # ğŸ¯ å…³é”®ä¿®å¤ï¼šä»rolloutè¯»å–å›ºå®šçš„ensemble sizeï¼Œé¿å…"æ··i"é—®é¢˜
                    # âœ… ä¸€æ¬¡rolloutå›ºå®šä¸€ä¸ªiï¼Œé˜²æ­¢step0ç”¨i=3ã€step1ç”¨i=9å¯¼è‡´çš„é”™ä½
                    rollout_ensemble_size = int(rollouts['ensemble_size'].item())

                    batch_new_log_probs = self.actor.log_prob_ensemble_rawmean(
                        batch_states, batch_raw_means,
                        ensemble_size=rollout_ensemble_size,
                        delta_std=self.ae_delta_std
                    )
                    # AEæ¨¡å¼ï¼šè·å–åˆ†å¸ƒç”¨äºç†µè®¡ç®—
                    mean, std = self.actor(batch_states)
                    if self.ae_delta_std is not None:
                        std = torch.ones_like(std) * self.ae_delta_std
                    dist = Normal(mean, std)
                else:
                    # ğŸ¯ æ ‡å‡†æ¨¡å¼ï¼šä½¿ç”¨raw_actionsé‡æ„log_probï¼ˆclip-onlyç‰ˆæœ¬ï¼‰
                    # ä»raw_meansé‡æ„log_probï¼Œç¡®ä¿ä¸rollouté˜¶æ®µçš„è®¡ç®—å®Œå…¨å¯¹é½
                    mean, std = self.actor(batch_states)
                    # AEæœªå¯ç”¨æ—¶ï¼Œstdä¿æŒç½‘ç»œè¾“å‡º
                    dist = Normal(mean, std)
                    batch_new_log_probs = dist.log_prob(batch_raw_means).sum(dim=-1)  # ğŸ¯ å¯¹raw_meansè®¡ç®—

                # âœ… KL early-stop (PPOæ ‡å‡†ç¨³å®šå™¨)
                # è®¡ç®—è¿‘ä¼¼KLæ•£åº¦ï¼šKL(pi_new || pi_old) â‰ˆ E[log pi_old - log pi_new]
                with torch.no_grad():
                    approx_kl = (batch_old_log_probs - batch_new_log_probs).mean().item()

                    # ğŸ›‘ KL early-stop: å¦‚æœKLè¶…è¿‡é˜ˆå€¼ï¼Œæå‰ç»“æŸæœ¬è½®update
                if approx_kl > 0.05:  # 0.01~0.03éƒ½è¡Œï¼Œ0.02æ˜¯æ¯”è¾ƒå¸¸ç”¨çš„å€¼
                    kl_early_stops += 1
                    if kl_early_stops == 1:  # åªåœ¨ç¬¬ä¸€æ¬¡è§¦å‘æ—¶æ‰“å°
                        print(f"âš ï¸ KL early-stop triggered: KL={approx_kl:.4f} > 0.02, stopping update epoch {update_epoch}")
                    stop_update = True  # âœ… è®¾ç½®æ ‡å¿—ï¼Œè·³å‡ºä¸¤å±‚å¾ªç¯
                    break  # è·³å‡ºå†…å±‚minibatchå¾ªç¯

                # è®¡ç®—æ¯”ç‡
                #ratio = torch.exp(batch_new_log_probs - batch_old_log_probs)
                batch_old_log_probs = torch.nan_to_num(batch_old_log_probs, nan=0.0, posinf=0.0, neginf=0.0)
                log_ratio = batch_new_log_probs - batch_old_log_probs
                log_ratio = torch.nan_to_num(log_ratio, nan=0.0, posinf=20.0, neginf=-20.0)
                log_ratio = torch.clamp(log_ratio, -10.0, 10.0)
                ratio = torch.exp(log_ratio)

                # ActoræŸå¤± (PPOè£å‰ª)
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * batch_advantages
                actor_loss = -torch.min(surr1, surr2).mean()

                # ç†µæ­£åˆ™åŒ–
                entropy = dist.entropy().sum(dim=-1).mean()
                # æŠŠç†µæ‰“åŒ…è¿› actor_loss
                actor_total_loss = actor_loss - self.entropy_coef * entropy

                # CriticæŸå¤±
                batch_values = self.critic(batch_states).squeeze(-1).float()
                normalized_returns = self.value_norm.normalize(batch_returns).float()
                critic_loss = F.mse_loss(batch_values, normalized_returns.detach())

                # æ€»æŸå¤±ï¼ˆç¡®ä¿ç†µé¡¹ç¬¦å·æ­£ç¡®ï¼‰
                loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy

                # æ¢¯åº¦è®¡ç®—éªŒè¯ (è°ƒè¯•æ¨¡å¼)
                if hasattr(self, '_debug_mode') and self._debug_mode:
                    # æ£€æŸ¥æŸå¤±æ˜¯å¦å¯ä»¥è®¡ç®—æ¢¯åº¦
                    if not loss.requires_grad:
                        print(f"âŒ æŸå¤±æ²¡æœ‰requires_grad: {loss.requires_grad}")

                    # æ£€æŸ¥actor_lossæ¢¯åº¦
                    if not actor_loss.requires_grad:
                        print(f"âŒ actor_lossæ²¡æœ‰requires_grad: {actor_loss.requires_grad}")

                    # æ£€æŸ¥ç½‘ç»œå‚æ•°æ¢¯åº¦
                    for name, param in self.actor.named_parameters():
                        if param.grad is not None:
                            if torch.isnan(param.grad).any():
                                print(f"âŒ Actorå‚æ•°{name}æ¢¯åº¦åŒ…å«NaN")
                        elif not param.requires_grad:
                            print(f"âŒ Actorå‚æ•°{name}ä¸éœ€è¦æ¢¯åº¦")

                # æ›´æ–°Actor
                self.actor_optimizer.zero_grad()

                try:
                    actor_total_loss.backward()

                    # æ£€æŸ¥actoræ¢¯åº¦
                    if hasattr(self, '_debug_mode') and self._debug_mode:
                        actor_grad_norm = sum(p.grad.norm().item() for p in self.actor.parameters() if p.grad is not None)
                        print(f"ğŸ“Š Actoræ¢¯åº¦èŒƒæ•°: {actor_grad_norm:.6f}")

                except Exception as e:
                    print(f"âŒ Actoræ¢¯åº¦è®¡ç®—å¤±è´¥: {e}")
                    raise

                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
                self.actor_optimizer.step()

                # æ›´æ–°Critic
                self.critic_optimizer.zero_grad()

                try:
                    critic_loss.backward()

                    # æ£€æŸ¥criticæ¢¯åº¦
                    if hasattr(self, '_debug_mode') and self._debug_mode:
                        critic_grad_norm = sum(p.grad.norm().item() for p in self.critic.parameters() if p.grad is not None)
                        print(f"ğŸ“Š Criticæ¢¯åº¦èŒƒæ•°: {critic_grad_norm:.6f}")

                except Exception as e:
                    print(f"âŒ Criticæ¢¯åº¦è®¡ç®—å¤±è´¥: {e}")
                    raise

                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
                self.critic_optimizer.step()

    
                # ç´¯è®¡ç»Ÿè®¡
                total_actor_loss += actor_loss.item()
                total_critic_loss += critic_loss.item()
                total_entropy += entropy.item()

            # âœ… æ£€æŸ¥å¤–å±‚early-stopæ ‡å¿—
            if stop_update:
                break

        # è®¡ç®—å¹³å‡å€¼
        num_updates = self.num_updates * (states.shape[0] // self.batch_size)

        # è·å–ç”¨äºæ˜¾ç¤ºçš„ç­–ç•¥std
        if self.ae_enabled:
            # AEæ¨¡å¼ï¼šä½¿ç”¨å›ºå®šÎ´_std
            policy_std = self.ae_delta_std
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šä½¿ç”¨æœ€åä¸€ä¸ªæ‰¹æ¬¡çš„ç½‘ç»œstd
            #policy_std = new_stds.mean().item()
             policy_std = float(std.mean().item())

        metrics = {
            'actor_loss': total_actor_loss / num_updates,
            'critic_loss': total_critic_loss / num_updates,
            'entropy': total_entropy / num_updates,
            'mean_advantage': advantages.mean().item(),
            'mean_return': returns.mean().item(),
            'policy_std': policy_std
        }

        return metrics

    def train(self, num_episodes: int = 1000, save_dir: str = "./checkpoints"):
        """
        å¼€å§‹è®­ç»ƒ

        Args:
            num_episodes: è®­ç»ƒå›åˆæ•°
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        import signal

        # è®¾ç½®é€€å‡ºä¿¡å·å¤„ç†
        shutdown_requested = False

        def signal_handler(signum, frame):
            nonlocal shutdown_requested
            if not shutdown_requested:
                print(f"\nğŸ›‘ æ¥æ”¶åˆ°é€€å‡ºä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
                shutdown_requested = True
            else:
                print(f"âš ï¸ å¼ºåˆ¶é€€å‡ºä¿¡å· {signum}ï¼Œç«‹å³é€€å‡º...")
                import sys
                sys.exit(1)

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡å›åˆæ•°: {num_episodes}")
        print(f"   æŒ‰ Ctrl+C å¯å®‰å…¨é€€å‡ºè®­ç»ƒ")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

        # ğŸ”¹ åˆå§‹åŒ– loss æ—¥å¿—æ–‡ä»¶
        loss_log_path = os.path.join(save_dir, "loss_curve.csv")
        with open(loss_log_path, "w") as f:
            f.write("log_step,episode,actor_loss,critic_loss,entropy,mean_return\n")

        # è®­ç»ƒç»Ÿè®¡
        training_stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'actor_losses': [],
            'critic_losses': [],
            'success_rates': []
        }

        start_time = time.time()

        for episode in range(num_episodes):
            # æ£€æŸ¥é€€å‡ºä¿¡å·
            if shutdown_requested:
                print(f"ğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨å®‰å…¨åœæ­¢è®­ç»ƒ...")
                print(f"   å·²å®Œæˆ {episode} ä¸ªå›åˆ")
                break

            # æ”¶é›†ç»éªŒ
            rollouts = self.collect_rollouts()

            # æ›´æ–°ç­–ç•¥
            metrics = self.update_policy(rollouts)

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            if episode % 10 == 0:
                avg_reward = self.best_performance
                current_time = time.time() - start_time

                print(f"ğŸ“Š Episode {episode:5d} | "
                      f"Best Reward: {avg_reward:8.4f} | "
                      f"Actor Loss: {metrics['actor_loss']:8.4f} | "
                      f"Critic Loss: {metrics['critic_loss']:8.4f} | "
                      f"Policy Std: {metrics['policy_std']:6.4f} | "
                      f"Time: {current_time/60:6.2f}min")
                
                 # ğŸ”¹ è¿½åŠ ä¸€è¡Œåˆ° CSVï¼ˆæ¯ 10 ä¸ª episode è®°ä¸€æ¬¡ï¼‰
                log_step = len(training_stats['actor_losses'])
                with open(loss_log_path, "a") as f:
                    f.write(
                        f"{log_step},{episode},"
                        f"{metrics['actor_loss']:.6f},{metrics['critic_loss']:.6f},"
                        f"{metrics['entropy']:.6f},{metrics['mean_return']:.6f}\n"
                    )

                # è®°å½•è®­ç»ƒç»Ÿè®¡
                training_stats['actor_losses'].append(metrics['actor_loss'])
                training_stats['critic_losses'].append(metrics['critic_loss'])

            # ä¿å­˜æ¨¡å‹
            if episode % 100 == 0 and episode > 0:
                self.save_model(save_dir, episode)

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time/60:.2f}åˆ†é’Ÿ")
        print(f"   æ€»å›åˆæ•°: {self.episode_count}")
        print(f"   æ€»æ­¥æ•°: {self.total_steps}")
        print(f"   æœ€ä½³æ€§èƒ½: {self.best_performance:.4f}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model(save_dir, 'final')

        return training_stats

    def save_model(self, save_dir: str, episode: int):
        """
        ä¿å­˜æ¨¡å‹

        Args:
            save_dir: ä¿å­˜ç›®å½•
            episode: å›åˆæ•°
        """
        os.makedirs(save_dir, exist_ok=True)

        checkpoint = {
            'episode': episode,
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'value_norm_state_dict': self.value_norm.state_dict(),
            'best_performance': self.best_performance,
            'episode_count': self.episode_count,
            'total_steps': self.total_steps
        }

        torch.save(checkpoint, os.path.join(save_dir, f'ppo_checkpoint_{episode}.pth'))

        # ä¹Ÿä¿å­˜ä¸ºæœ€æ–°ç‰ˆæœ¬
        torch.save(checkpoint, os.path.join(save_dir, 'latest.pth'))

    def load_model(self, checkpoint_path: str):
        """
        åŠ è½½æ¨¡å‹

        Args:
            checkpoint_path: æ¨¡å‹è·¯å¾„
        """
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.value_norm.load_state_dict(checkpoint['value_norm_state_dict'])

        self.best_performance = checkpoint['best_performance']
        self.episode_count = checkpoint['episode_count']
        self.total_steps = checkpoint['total_steps']

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå›åˆæ•°: {checkpoint['episode']}")

def load_config_isaac(config_path: str = "config.yaml") -> Dict[str, Any]:
    """
    åŠ è½½Isaac Gymç‰ˆæœ¬é…ç½®

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        config: é…ç½®å­—å…¸
    """
    import yaml
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = get_default_config_isaac()

    return config

def get_default_config_isaac() -> Dict[str, Any]:
    """è·å–é»˜è®¤Isaac Gymé…ç½®"""
    return {
        'env': {
            'num_envs': 512,
            'max_steps': 1000,
            'dt': 0.01
        },
        'ppo': {
            'lr_actor': 3e-4,
            'lr_critic': 1e-3,
            'clip_eps': 0.2,
            'gamma': 0.99,
            'lam': 0.95,
            'entropy_coef': 0.001,  # å¤§å¹…å‡å° entropy ç³»æ•°
            'value_coef': 0.5,
            'max_grad_norm': 0.5
        },
        'train': {
            'rollout_length': 2048,
            'batch_size': 512,
            'num_updates': 10,
            'num_episodes': 1000
        }
    }

if __name__ == "__main__":
    # åŠ è½½é…ç½®
    config = load_config_isaac()

    # åˆ›å»ºç¯å¢ƒ
    env = UR10ePPOEnvIsaac(
        config_path="config.yaml",
        num_envs=config['env']['num_envs']
    )

    # åˆ›å»ºPPOè®­ç»ƒå™¨
    ppo = PPOIsaac(env, config)

    # å¼€å§‹è®­ç»ƒ
    training_stats = ppo.train(
        num_episodes=config['train']['num_episodes'],
        save_dir="./checkpoints_isaac"
    )

    # å…³é—­ç¯å¢ƒ
    env.close()