"""
PPO (Proximal Policy Optimization) Implementation - Isaac Gymç‰ˆæœ¬

é’ˆå¯¹Isaac Gymä¼˜åŒ–çš„PPOå®ç°ï¼Œæ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒ
é›†æˆRL-PIDæ··åˆæ§åˆ¶å’Œå¥–åŠ±å½’ä¸€åŒ–åŠŸèƒ½
"""

# IMPORTANT: Isaac Gym must be imported before PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal
import numpy as np
import gym
from typing import Dict, Any, List, Tuple, Optional
import time
import os

from ur10e_env_isaac import UR10ePPOEnvIsaac
from utils import (ValueNormalization, GAE, assert_same_device, check_tensor_devices,
                   get_tensor_device, ensure_device, get_forced_device)


class ActorNetwork(nn.Module):
    """Actorç½‘ç»œ - PPOç­–ç•¥å‡½æ•°"""

    def __init__(self, state_dim: int = 18, action_dim: int = 6, hidden_dim: int = 64):
        super().__init__()

        self.state_dim = state_dim
        self.action_dim = action_dim

        self.max_torques = np.array([264.0, 264.0, 120.0, 43.2, 43.2, 43.2], dtype=np.float32)
        self.action_space_high = self.max_torques
        self.action_space_low = -self.max_torques

        self.register_buffer("max_torques_tensor", torch.tensor(self.max_torques, dtype=torch.float32)) 

        # ç­–ç•¥ç½‘ç»œ
        self.policy_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim * 2)  # å‡å€¼å’Œæ ‡å‡†å·®
        )

        # åˆå§‹åŒ–æƒé‡
        self._init_actor_weights()

    def _init_actor_weights(self):
        """æ ‡å‡†çš„ Orthogonal åˆå§‹åŒ– + é›¶åç½®ï¼Œä¸å†ç»™è¾“å‡ºå±‚åŠ  1.05 åç½®"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
                nn.init.constant_(m.bias, 0.0)


    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            state: [batch_size, state_dim] çŠ¶æ€å¼ é‡

        Returns:
            mean: [batch_size, action_dim] åŠ¨ä½œå‡å€¼
            std: [batch_size, action_dim] åŠ¨ä½œæ ‡å‡†å·®
        """
        #self.max_torques_tensor = torch.tensor(self.max_torques, device=self.device, dtype=torch.float32)

        policy_output = self.policy_net(state)
        mean, log_std = policy_output.chunk(2, dim=-1)

        log_std = torch.clamp(log_std, -2.0, 2.0)
        #std = F.softplus(log_std)   # ç¡®ä¿æ ‡å‡†å·®ä¸ºæ­£
        #softplus(x) = log(1 + exp(x))
        std = torch.exp(log_std) 
        return mean, std

    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        é‡‡æ ·åŠ¨ä½œ

        Args:
            state: [batch_size, state_dim] çŠ¶æ€å¼ é‡

        Returns:
            action: [batch_size, action_dim] é‡‡æ ·åŠ¨ä½œ
            log_prob: [batch_size] å¯¹æ•°æ¦‚ç‡
        """
        mean, std = self.forward(state)
        dist = Normal(mean, std)

        #action = dist.sample() 
        raw = dist.rsample()  # ç”¨ rsample æ–¹ä¾¿ä»¥ååš reparameterization
        log_prob = dist.log_prob(raw).sum(dim=-1)

        action = torch.tanh(raw)  # å°†åŠ¨ä½œé™åˆ¶åœ¨[-1, 1]èŒƒå›´å†…
        action = action * self.max_torques_tensor

        #max_tau = 30.0PPOIsaac.collect_rollouts
        #action = torch.clamp(action, -max_tau, max_tau)

        return action, log_prob

class CriticNetwork(nn.Module):
    """Criticç½‘ç»œ - PPOä»·å€¼å‡½æ•°"""

    def __init__(self, state_dim: int = 18, hidden_dim: int = 64):
        super().__init__()

        # ä»·å€¼ç½‘ç»œ
        self.value_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight.data, gain=np.sqrt(2))
            nn.init.constant_(module.bias.data, 0)

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            state: [batch_size, state_dim] çŠ¶æ€å¼ é‡

        Returns:
            value: [batch_size, 1] çŠ¶æ€ä»·å€¼
        """
        return self.value_net(state)

class PPOIsaac:
    """
    PPOç®—æ³•å®ç° - Isaac Gymç‰ˆæœ¬

    ä¸“é—¨é’ˆå¯¹Isaac Gymå¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒä¼˜åŒ–
    é›†æˆä»·å€¼å½’ä¸€åŒ–ã€GAEã€æ¢¯åº¦è£å‰ªç­‰æŠ€æœ¯
    """

    def __init__(self,
                 env: UR10ePPOEnvIsaac,
                 config: Dict[str, Any]):
        """
        åˆå§‹åŒ–PPOè®­ç»ƒå™¨

        Args:
            env: Isaac Gymç¯å¢ƒ
            config: è®­ç»ƒé…ç½®
        """
        self.env = env
        self.config = config

        # ğŸ¯ [SERVER FIX] ä½¿ç”¨ç¯å¢ƒè®¾å¤‡ï¼ˆå‚è€ƒisaac_gym_manipulatorå®ç°æ¨¡å¼ï¼‰
        self.device = env.device
        print(f"ğŸ”’ [ENV DEVICE] PPOä½¿ç”¨ç¯å¢ƒè®¾å¤‡: {self.device}")

        # ğŸš¨ [SERVER SAFETY] éªŒè¯è®¾å¤‡ä¸€è‡´æ€§
        forced_device = get_forced_device()
        if str(self.device) != str(forced_device):
            print(f"âš ï¸ [DEVICE MISMATCH] ç¯å¢ƒè®¾å¤‡{self.device} != å¼ºåˆ¶è®¾å¤‡{forced_device}")
            print(f"   å¼ºåˆ¶ä½¿ç”¨ç¯å¢ƒè®¾å¤‡: {self.device}")
            # åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­ï¼Œå¼ºåˆ¶ç¯å¢ƒè®¾å¤‡ä¸ºcuda:0
            if torch.cuda.is_available() and torch.cuda.device_count() > 1:
                os.environ['CUDA_VISIBLE_DEVICES'] = '0'

        # ç¯å¢ƒå‚æ•°
        self.num_envs = env.get_num_envs()
        self.state_dim = env.get_num_obs()
        self.action_dim = env.get_num_actions()

        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = ActorNetwork(self.state_dim, self.action_dim).to(self.device)
        self.critic = CriticNetwork(self.state_dim).to(self.device)

        # ç¡®ä¿ç½‘ç»œå‚æ•°åœ¨æ­£ç¡®è®¾å¤‡ä¸Š (ä¿®å¤è®¾å¤‡ä¸€è‡´æ€§)
        assert next(self.actor.parameters()).device == self.device, "Actor not on correct device"
        assert next(self.critic.parameters()).device == self.device, "Critic not on correct device"

        # æ˜¾å¼è®¾ç½®ç½‘ç»œå‚æ•°requires_grad=True
        for param in self.actor.parameters():
            param.requires_grad = True
        for param in self.critic.parameters():
            param.requires_grad = True

        # éªŒè¯æ¢¯åº¦è®¾ç½®
        assert all(p.requires_grad for p in self.actor.parameters()), "Actorå‚æ•°æœªè®¾ç½®requires_grad"
        assert all(p.requires_grad for p in self.critic.parameters()), "Criticå‚æ•°æœªè®¾ç½®requires_grad"

        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(
            self.actor.parameters(),
            lr=float(config['ppo']['lr_actor'])
        )
        self.critic_optimizer = optim.Adam(
            self.critic.parameters(),
            lr=float(config['ppo']['lr_critic'])
        )

        # ä»·å€¼å½’ä¸€åŒ–å™¨
        self.value_norm = ValueNormalization(
            beta=0.995,
            epsilon=1e-5,
            clip_range=10.0
        ).to(self.device)

        # GAEè®¡ç®—å™¨
        self.gae = GAE(
            gamma=float(config['ppo']['gamma']),
            lam=float(config['ppo']['lam'])
        )

        # è®­ç»ƒå‚æ•° - ç¡®ä¿ç±»å‹è½¬æ¢
        self.clip_eps = float(config['ppo']['clip_eps'])
        self.entropy_coef = float(config['ppo']['entropy_coef'])
        self.value_coef = float(config['ppo']['value_coef'])
        self.max_grad_norm = float(config['ppo']['max_grad_norm'])

        # ç¼“å†²åŒºå‚æ•° - ç¡®ä¿æ•´æ•°ç±»å‹
        self.rollout_length = int(config['train']['rollout_length'])
        self.batch_size = int(config['train']['batch_size'])
        self.num_updates = int(config['train']['num_updates'])

        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_count = 0
        self.total_steps = 0
        self.best_performance = -float('inf')

        print(f"ğŸ¤– Isaac Gym PPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   å¹¶è¡Œç¯å¢ƒæ•°: {self.num_envs}")
        print(f"   çŠ¶æ€ç»´åº¦: {self.state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"   è®¾å¤‡: {self.device}")

        # æ¢¯åº¦è®¡ç®—æµ‹è¯•
        if not self._test_gradient_flow():
            print("âŒ æ¢¯åº¦è®¡ç®—æµ‹è¯•å¤±è´¥")
            raise RuntimeError("æ¢¯åº¦è®¡ç®—æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå®ç°")

    def _test_gradient_flow(self):
        """æµ‹è¯•æ¢¯åº¦è®¡ç®—æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_states = torch.randn(2, self.state_dim, device=self.device, requires_grad=True)
            test_actions = torch.randn(2, self.action_dim, device=self.device)
            test_old_log_probs = torch.randn(2, device=self.device)
            test_advantages = torch.randn(2, device=self.device)

            # æµ‹è¯•actoræ¢¯åº¦
            new_means, new_stds = self.actor(test_states)
            dist = Normal(new_means, new_stds)
            new_log_probs = dist.log_prob(test_actions).sum(dim=-1)
            ratio = torch.exp(new_log_probs - test_old_log_probs)
            actor_loss = -(ratio * test_advantages).mean()

            assert actor_loss.requires_grad, "ActoræŸå¤±æ²¡æœ‰æ¢¯åº¦"
            actor_loss.backward()

            # æ£€æŸ¥actorå‚æ•°æ¢¯åº¦
            actor_has_grad = any(p.grad is not None for p in self.actor.parameters())
            assert actor_has_grad, "Actorå‚æ•°æ²¡æœ‰æ¢¯åº¦"

            # æµ‹è¯•criticæ¢¯åº¦
            test_values = self.critic(test_states).squeeze(-1)
            test_returns = torch.randn(2, device=self.device)
            critic_loss = F.mse_loss(test_values, test_returns)

            assert critic_loss.requires_grad, "CriticæŸå¤±æ²¡æœ‰æ¢¯åº¦"
            critic_loss.backward()

            # æ£€æŸ¥criticå‚æ•°æ¢¯åº¦
            critic_has_grad = any(p.grad is not None for p in self.critic.parameters())
            assert critic_has_grad, "Criticå‚æ•°æ²¡æœ‰æ¢¯åº¦"

            # æ¸…ç†æ¢¯åº¦
            self.actor_optimizer.zero_grad()
            self.critic_optimizer.zero_grad()

            print("âœ… æ¢¯åº¦è®¡ç®—æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            print(f"âŒ æ¢¯åº¦è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            return False

    def collect_rollouts(self) -> Dict[str, torch.Tensor]:
        """
        æ”¶é›†ç»éªŒå›æ”¾æ•°æ®

        Returns:
            rollouts: æ”¶é›†çš„æ•°æ®å­—å…¸
        """
        # é‡ç½®ç¯å¢ƒ
        states = self.env.reset()
        # ç¡®ä¿çŠ¶æ€åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        states = ensure_device(states, self.device)

        # åˆå§‹åŒ–ç¼“å†²åŒº
        rollouts = {
            'states': [],
            'actions': [],
            'log_probs': [],
            'values': [],
            'rewards': [],
            'dones': [],
            'next_states': []
        }

        episode_rewards = np.zeros(self.num_envs)
        episode_lengths = np.zeros(self.num_envs)

        for step in range(self.rollout_length):
            # è®°å½•å½“å‰çŠ¶æ€
            rollouts['states'].append(states.clone())

            # é‡‡æ ·åŠ¨ä½œ (æ•°æ®æ”¶é›†æ—¶ä½¿ç”¨no_gradï¼Œä½†çŠ¶æ€éœ€è¦æ¢¯åº¦)
            states_for_sampling = states.detach().requires_grad_(True)
            with torch.no_grad():
                actions, log_probs = self.actor.sample(states_for_sampling)
                values = self.critic(states_for_sampling)

            # è°ƒè¯•ä¿¡æ¯ (æ¯64æ­¥æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦)
            if step % 64 == 0 and hasattr(self.env, 'episode_steps'):
                avg_episode_steps = self.env.episode_steps.mean().item()
                max_episode_steps = self.env.episode_steps.max().item()
                print(f"ğŸ“ˆ Step {step:3d}: å¹³å‡episodeæ­¥æ•°: {avg_episode_steps:.1f}, æœ€å¤§: {max_episode_steps}")

            # æ‰§è¡ŒåŠ¨ä½œ
            next_states, rewards, dones, infos = self.env.step(actions)

            # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            next_states = ensure_device(next_states, self.device)
            rewards = ensure_device(rewards, self.device)
            dones = ensure_device(dones, self.device)

            # è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥ (ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜)
            try:
                assert_same_device(states, actions, next_states, rewards, dones, device=self.device)
            except AssertionError as e:
                print(f"âŒ è®¾å¤‡ä¸åŒ¹é…é”™è¯¯: {e}")
                print(f"   states: {states.device}")
                print(f"   actions: {actions.device}")
                print(f"   next_states: {next_states.device}")
                print(f"   rewards: {rewards.device}")
                print(f"   dones: {dones.device}")
                print(f"   æœŸæœ›è®¾å¤‡: {self.device}")
                raise

            # è®°å½•æ•°æ®
            rollouts['actions'].append(actions.clone())
            rollouts['log_probs'].append(log_probs.clone())
            rollouts['values'].append(values.squeeze(-1).clone())
            rollouts['rewards'].append(rewards.clone())
            rollouts['dones'].append(dones.clone())
            rollouts['next_states'].append(next_states.clone())

            # ç»Ÿè®¡ä¿¡æ¯ (ä¿®å¤è®¾å¤‡ä¸åŒ¹é…)
            rewards_device = rewards.device
            episode_rewards += rewards.detach().cpu().numpy()
            episode_lengths += 1

            # è®¾å¤‡ä¸€è‡´æ€§è°ƒè¯•ä¿¡æ¯
            if rewards_device != self.device:
                print(f"âš ï¸ è®¾å¤‡ä¸åŒ¹é…: rewardsåœ¨{rewards_device}, æœŸæœ›åœ¨{self.device}")

            # å¤„ç†å®Œæˆçš„å›åˆ
            for i in range(self.num_envs):
                if dones[i]:
                    self.episode_count += 1
                    self.total_steps += episode_lengths[i]

                    print(f"ğŸ¯ Episodeå®Œæˆ! ç¯å¢ƒ{i}, å¥–åŠ±: {episode_rewards[i]:.4f}, æ­¥æ•°: {episode_lengths[i]}")

                    # æ›´æ–°æœ€ä½³æ€§èƒ½
                    if episode_rewards[i] > self.best_performance:
                        self.best_performance = episode_rewards[i]
                        print(f"ğŸ† æ–°æœ€ä½³æ€§èƒ½! {self.best_performance:.4f}")

                    # é‡ç½®ç»Ÿè®¡
                    episode_rewards[i] = 0
                    episode_lengths[i] = 0

            states = next_states

        # è½¬æ¢ä¸ºå¼ é‡
        for key in rollouts:
            if key != 'next_states':
                rollouts[key] = torch.stack(rollouts[key], dim=0)  # [rollout_length, num_envs]

        return rollouts

    def update_policy(self, rollouts: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        æ›´æ–°ç­–ç•¥ç½‘ç»œ

        Args:
            rollouts: ç»éªŒå›æ”¾æ•°æ®

        Returns:
            metrics: è®­ç»ƒæŒ‡æ ‡
        """
        # ğŸ¯ [SERVER FIX] ç¡®ä¿æ‰€æœ‰rolloutæ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        for key, tensor in rollouts.items():
            if isinstance(tensor, torch.Tensor) and tensor.device != self.device:
                print(f"âš ï¸ [DEVICE FIX] {key}ä»{tensor.device}ç§»åŠ¨åˆ°{self.device}")
                rollouts[key] = tensor.to(self.device)

        # å‡†å¤‡æ•°æ®
        states = rollouts['states'].view(-1, self.state_dim)  # [T*N, state_dim]
        actions = rollouts['actions'].view(-1, self.action_dim)  # [T*N, action_dim]
        old_log_probs = rollouts['log_probs'].view(-1)  # [T*N]

        # è®¡ç®—ä»·å€¼å’Œä¼˜åŠ¿
        values = rollouts['values'].view(self.rollout_length, self.num_envs)  # [T, N]
        rewards = rollouts['rewards'].view(self.rollout_length, self.num_envs)  # [T, N]
        dones = rollouts['dones'].view(self.rollout_length, self.num_envs)  # [T, N]

        # ğŸ” [CRITICAL CHECK] éªŒè¯è®¾å¤‡ä¸€è‡´æ€§ï¼ˆé¢„é˜²ç¬¬500æ­¥é”™è¯¯ï¼‰
        try:
            assert_same_device(states, actions, old_log_probs, values, rewards, dones, device=self.device)
            print(f"âœ… [DEVICE OK] æ‰€æœ‰rolloutæ•°æ®åœ¨{self.device}ä¸Š")
        except AssertionError as e:
            print(f"âŒ [DEVICE ERROR] {e}")
            # å¼ºåˆ¶ä¿®å¤
            states = states.to(self.device)
            actions = actions.to(self.device)
            old_log_probs = old_log_probs.to(self.device)
            values = values.to(self.device)
            rewards = rewards.to(self.device)
            dones = dones.to(self.device)

        # è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼
        with torch.no_grad():
            last_next_state = rollouts['next_states'][-1].to(self.device)
            next_values = self.critic(last_next_state).squeeze(-1)  # [N]
            # ä¿®å¤ï¼šä¸ºGAEå‡½æ•°åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„next_values [T, N]
            next_values_expanded = next_values.unsqueeze(0).expand(self.rollout_length, -1)  # [T, N]

        # è®¡ç®—GAEä¼˜åŠ¿å’Œå›æŠ¥
        advantages, returns = self.gae(rewards, dones, values, next_values_expanded)

        # å±•å¹³
        advantages = advantages.view(-1).float()  # [T*N]
        returns = returns.view(-1).float()  # [T*N]

        # å½’ä¸€åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # æ›´æ–°ä»·å€¼å½’ä¸€åŒ–å™¨
        self.value_norm.update(returns)

        # å¤šæ¬¡æ›´æ–°
        total_actor_loss = 0
        total_critic_loss = 0
        total_entropy = 0

        for _ in range(self.num_updates):
            # éšæœºé‡‡æ ·æ‰¹æ¬¡
            indices = torch.randperm(states.shape[0])

            for start in range(0, states.shape[0], self.batch_size):
                end = min(start + self.batch_size, states.shape[0])
                batch_indices = indices[start:end]

                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_returns = returns[batch_indices]

                # è®¡ç®—æ–°çš„åŠ¨ä½œæ¦‚ç‡ (éœ€è¦æ¢¯åº¦è¿›è¡Œæ›´æ–°)
                #new_means, new_stds = self.actor(batch_states)
                #ist = Normal(new_means, new_stds)
                #batch_new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)

                # è®¡ç®—æ¯”ç‡
                #ratio = torch.exp(batch_new_log_probs - batch_old_log_probs)

                # 1. å¾—åˆ°é«˜æ–¯å‚æ•°ï¼ˆraw ç©ºé—´ï¼‰
                new_means, new_stds = self.actor(batch_states)   # [B, act_dim]
                dist = Normal(new_means, new_stds)

                # 2. æŠŠæ‰­çŸ©åŠ¨ä½œè¿˜åŸå› raw ç©ºé—´
                #   2.1 å…ˆé™¤ä»¥ max_torques å¾—åˆ° squashed âˆˆ [-1,1]
                max_torques = self.actor.max_torques_tensor      # [6]
                squashed = batch_actions / max_torques           # [B,6]ï¼Œè‡ªåŠ¨ broadcast

                #   2.2 æ•°å€¼å®‰å…¨ä¸€ç‚¹ï¼Œå¤¹ç´§åœ¨ (-1+eps, 1-eps)
                eps = 1e-6
                squashed = torch.clamp(squashed, -1.0 + eps, 1.0 - eps)

                #   2.3 å tanhï¼šraw = atanh(squashed)
                raw = 0.5 * (torch.log1p(squashed) - torch.log1p(-squashed))
                # ä¹Ÿå¯ä»¥ç”¨ torch.atanh(squashed)ï¼ˆå¦‚æœä½ çš„ torch ç‰ˆæœ¬æ”¯æŒï¼‰

                # 3. åœ¨ raw ç©ºé—´ä¸‹ç®— log_prob
                batch_new_log_probs = dist.log_prob(raw).sum(dim=-1)

                # 4. ä¸€åˆ‡ç…§æ—§
                ratio = torch.exp(batch_new_log_probs - batch_old_log_probs)


                # ActoræŸå¤± (PPOè£å‰ª)
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * batch_advantages
                actor_loss = -torch.min(surr1, surr2).mean()

                # ç†µæ­£åˆ™åŒ–
                entropy = dist.entropy().sum(dim=-1).mean()

                # CriticæŸå¤±
                batch_values = self.critic(batch_states).squeeze(-1).float()
                normalized_returns = self.value_norm.normalize(batch_returns).float()
                critic_loss = F.mse_loss(batch_values, normalized_returns.detach())

                # æ€»æŸå¤±
                loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy

                # æ¢¯åº¦è®¡ç®—éªŒè¯ (è°ƒè¯•æ¨¡å¼)
                if hasattr(self, '_debug_mode') and self._debug_mode:
                    # æ£€æŸ¥æŸå¤±æ˜¯å¦å¯ä»¥è®¡ç®—æ¢¯åº¦
                    if not loss.requires_grad:
                        print(f"âŒ æŸå¤±æ²¡æœ‰requires_grad: {loss.requires_grad}")

                    # æ£€æŸ¥actor_lossæ¢¯åº¦
                    if not actor_loss.requires_grad:
                        print(f"âŒ actor_lossæ²¡æœ‰requires_grad: {actor_loss.requires_grad}")

                    # æ£€æŸ¥ç½‘ç»œå‚æ•°æ¢¯åº¦
                    for name, param in self.actor.named_parameters():
                        if param.grad is not None:
                            if torch.isnan(param.grad).any():
                                print(f"âŒ Actorå‚æ•°{name}æ¢¯åº¦åŒ…å«NaN")
                        elif not param.requires_grad:
                            print(f"âŒ Actorå‚æ•°{name}ä¸éœ€è¦æ¢¯åº¦")

                # æ›´æ–°Actor
                self.actor_optimizer.zero_grad()

                try:
                    actor_loss.backward(retain_graph=True)

                    # æ£€æŸ¥actoræ¢¯åº¦
                    if hasattr(self, '_debug_mode') and self._debug_mode:
                        actor_grad_norm = sum(p.grad.norm().item() for p in self.actor.parameters() if p.grad is not None)
                        print(f"ğŸ“Š Actoræ¢¯åº¦èŒƒæ•°: {actor_grad_norm:.6f}")

                except Exception as e:
                    print(f"âŒ Actoræ¢¯åº¦è®¡ç®—å¤±è´¥: {e}")
                    raise

                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
                self.actor_optimizer.step()

                # æ›´æ–°Critic
                self.critic_optimizer.zero_grad()

                try:
                    critic_loss.backward()

                    # æ£€æŸ¥criticæ¢¯åº¦
                    if hasattr(self, '_debug_mode') and self._debug_mode:
                        critic_grad_norm = sum(p.grad.norm().item() for p in self.critic.parameters() if p.grad is not None)
                        print(f"ğŸ“Š Criticæ¢¯åº¦èŒƒæ•°: {critic_grad_norm:.6f}")

                except Exception as e:
                    print(f"âŒ Criticæ¢¯åº¦è®¡ç®—å¤±è´¥: {e}")
                    raise

                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
                self.critic_optimizer.step()

                # ç´¯è®¡ç»Ÿè®¡
                total_actor_loss += actor_loss.item()
                total_critic_loss += critic_loss.item()
                total_entropy += entropy.item()

        # è®¡ç®—å¹³å‡å€¼
        num_updates = self.num_updates * (states.shape[0] // self.batch_size)

        metrics = {
            'actor_loss': total_actor_loss / num_updates,
            'critic_loss': total_critic_loss / num_updates,
            'entropy': total_entropy / num_updates,
            'mean_advantage': advantages.mean().item(),
            'mean_return': returns.mean().item(),
            'policy_std': new_stds.mean().item()
        }

        return metrics

    def train(self, num_episodes: int = 1000, save_dir: str = "./checkpoints"):
        """
        å¼€å§‹è®­ç»ƒ

        Args:
            num_episodes: è®­ç»ƒå›åˆæ•°
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        import signal

        # è®¾ç½®é€€å‡ºä¿¡å·å¤„ç†
        shutdown_requested = False

        def signal_handler(signum, frame):
            nonlocal shutdown_requested
            if not shutdown_requested:
                print(f"\nğŸ›‘ æ¥æ”¶åˆ°é€€å‡ºä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
                shutdown_requested = True
            else:
                print(f"âš ï¸ å¼ºåˆ¶é€€å‡ºä¿¡å· {signum}ï¼Œç«‹å³é€€å‡º...")
                import sys
                sys.exit(1)

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡å›åˆæ•°: {num_episodes}")
        print(f"   æŒ‰ Ctrl+C å¯å®‰å…¨é€€å‡ºè®­ç»ƒ")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

        # ğŸ”¹ åˆå§‹åŒ– loss æ—¥å¿—æ–‡ä»¶
        loss_log_path = os.path.join(save_dir, "loss_curve.csv")
        with open(loss_log_path, "w") as f:
            f.write("log_step,episode,actor_loss,critic_loss,entropy,mean_return\n")

        # è®­ç»ƒç»Ÿè®¡
        training_stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'actor_losses': [],
            'critic_losses': [],
            'success_rates': []
        }

        start_time = time.time()

        for episode in range(num_episodes):
            # æ£€æŸ¥é€€å‡ºä¿¡å·
            if shutdown_requested:
                print(f"ğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨å®‰å…¨åœæ­¢è®­ç»ƒ...")
                print(f"   å·²å®Œæˆ {episode} ä¸ªå›åˆ")
                break

            # æ”¶é›†ç»éªŒ
            rollouts = self.collect_rollouts()

            # æ›´æ–°ç­–ç•¥
            metrics = self.update_policy(rollouts)

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            if episode % 10 == 0:
                avg_reward = self.best_performance
                current_time = time.time() - start_time

                print(f"ğŸ“Š Episode {episode:5d} | "
                      f"Best Reward: {avg_reward:8.4f} | "
                      f"Actor Loss: {metrics['actor_loss']:8.4f} | "
                      f"Critic Loss: {metrics['critic_loss']:8.4f} | "
                      f"Policy Std: {metrics['policy_std']:6.4f} | "
                      f"Time: {current_time/60:6.2f}min")
                
                 # ğŸ”¹ è¿½åŠ ä¸€è¡Œåˆ° CSVï¼ˆæ¯ 10 ä¸ª episode è®°ä¸€æ¬¡ï¼‰
                log_step = len(training_stats['actor_losses'])
                with open(loss_log_path, "a") as f:
                    f.write(
                        f"{log_step},{episode},"
                        f"{metrics['actor_loss']:.6f},{metrics['critic_loss']:.6f},"
                        f"{metrics['entropy']:.6f},{metrics['mean_return']:.6f}\n"
                    )

                # è®°å½•è®­ç»ƒç»Ÿè®¡
                training_stats['actor_losses'].append(metrics['actor_loss'])
                training_stats['critic_losses'].append(metrics['critic_loss'])

            # ä¿å­˜æ¨¡å‹
            if episode % 100 == 0 and episode > 0:
                self.save_model(save_dir, episode)

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time/60:.2f}åˆ†é’Ÿ")
        print(f"   æ€»å›åˆæ•°: {self.episode_count}")
        print(f"   æ€»æ­¥æ•°: {self.total_steps}")
        print(f"   æœ€ä½³æ€§èƒ½: {self.best_performance:.4f}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model(save_dir, 'final')

        return training_stats

    def save_model(self, save_dir: str, episode: int):
        """
        ä¿å­˜æ¨¡å‹

        Args:
            save_dir: ä¿å­˜ç›®å½•
            episode: å›åˆæ•°
        """
        os.makedirs(save_dir, exist_ok=True)

        checkpoint = {
            'episode': episode,
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'value_norm_state_dict': self.value_norm.state_dict(),
            'best_performance': self.best_performance,
            'episode_count': self.episode_count,
            'total_steps': self.total_steps
        }

        torch.save(checkpoint, os.path.join(save_dir, f'ppo_checkpoint_{episode}.pth'))

        # ä¹Ÿä¿å­˜ä¸ºæœ€æ–°ç‰ˆæœ¬
        torch.save(checkpoint, os.path.join(save_dir, 'latest.pth'))

    def load_model(self, checkpoint_path: str):
        """
        åŠ è½½æ¨¡å‹

        Args:
            checkpoint_path: æ¨¡å‹è·¯å¾„
        """
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.value_norm.load_state_dict(checkpoint['value_norm_state_dict'])

        self.best_performance = checkpoint['best_performance']
        self.episode_count = checkpoint['episode_count']
        self.total_steps = checkpoint['total_steps']

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå›åˆæ•°: {checkpoint['episode']}")

def load_config_isaac(config_path: str = "config.yaml") -> Dict[str, Any]:
    """
    åŠ è½½Isaac Gymç‰ˆæœ¬é…ç½®

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        config: é…ç½®å­—å…¸
    """
    import yaml
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = get_default_config_isaac()

    return config

def get_default_config_isaac() -> Dict[str, Any]:
    """è·å–é»˜è®¤Isaac Gymé…ç½®"""
    return {
        'env': {
            'num_envs': 512,
            'max_steps': 1000,
            'dt': 0.01
        },
        'ppo': {
            'lr_actor': 3e-4,
            'lr_critic': 1e-3,
            'clip_eps': 0.2,
            'gamma': 0.99,
            'lam': 0.95,
            'entropy_coef': 0.01,
            'value_coef': 0.5,
            'max_grad_norm': 0.5
        },
        'train': {
            'rollout_length': 2048,
            'batch_size': 512,
            'num_updates': 10,
            'num_episodes': 1000
        }
    }

if __name__ == "__main__":
    # åŠ è½½é…ç½®
    config = load_config_isaac()

    # åˆ›å»ºç¯å¢ƒ
    env = UR10ePPOEnvIsaac(
        config_path="config.yaml",
        num_envs=config['env']['num_envs']
    )

    # åˆ›å»ºPPOè®­ç»ƒå™¨
    ppo = PPOIsaac(env, config)

    # å¼€å§‹è®­ç»ƒ
    training_stats = ppo.train(
        num_episodes=config['train']['num_episodes'],
        save_dir="./checkpoints_isaac"
    )

    # å…³é—­ç¯å¢ƒ
    env.close()