"""
UR10e PPO å¤šç›®æ ‡æœ€ä¼˜è½¨è¿¹è§„åˆ’å·¥å…·å‡½æ•°

åŸºäºè®ºæ–‡ã€ŠåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æœºæ¢°è‡‚å¤šç›®æ ‡æœ€ä¼˜è½¨è¿¹è§„åˆ’ã€‹
æä¾›è®­ç»ƒã€è¯„ä¼°å’Œå¯è§†åŒ–çš„è¾…åŠ©åŠŸèƒ½
"""

import numpy as np
import torch
import torch.nn as nn
import yaml
import os
import json
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime
import math

# å¯é€‰ä¾èµ– - åªæœ‰åœ¨éœ€è¦æ—¶æ‰å¯¼å…¥
try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False


def assert_same_device(*tensors, device=None):
    """
    ç¡®ä¿æ‰€æœ‰tensoråœ¨åŒä¸€è®¾å¤‡ä¸Š

    Args:
        *tensors: è¦æ£€æŸ¥çš„tensors
        device: æœŸæœ›çš„è®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªtensorçš„è®¾å¤‡
    """
    if len(tensors) == 0:
        return

    devices = [t.device for t in tensors if hasattr(t, 'device')]
    if not devices:
        return

    target_device = device if device else devices[0]

    for i, tensor in enumerate(tensors):
        if hasattr(tensor, 'device') and tensor.device != target_device:
            raise AssertionError(f"Tensor {i} on {tensor.device}, expected {target_device}")


class RewardNormalizer:
    """
    å¥–åŠ±å½’ä¸€åŒ–å™¨ - ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
    """
    def __init__(self, gamma=0.99, clip_range=5.0, normalize_method='running_stats', warmup_steps=100, device='cuda'):
        self.gamma = gamma
        self.clip_range = clip_range
        self.normalize_method = normalize_method
        self.warmup_steps = warmup_steps
        self.device = device

        # è¿è¡Œç»Ÿè®¡é‡ - åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        self.register_buffer = lambda name, tensor: setattr(self, name, tensor)
        self.register_buffer('running_mean', torch.zeros(1, device=device))
        self.register_buffer('running_var', torch.ones(1, device=device))
        self.register_buffer('count', torch.zeros(1, device=device))

        self.reward_history = []

    def update(self, reward):
        """æ›´æ–°å½’ä¸€åŒ–ç»Ÿè®¡é‡"""
        reward = float(reward)
        self.reward_history.append(reward)

        # ä¿æŒå†å²é•¿åº¦
        if len(self.reward_history) > 1000:
            self.reward_history = self.reward_history[-1000:]

        if len(self.reward_history) >= self.warmup_steps:
            # æ›´æ–°è¿è¡Œç»Ÿè®¡
            recent_mean = np.mean(self.reward_history[-100:])
            recent_std = np.std(self.reward_history[-100:]) + 1e-8

            self.running_mean.copy_(torch.tensor([recent_mean], device=self.device))
            self.running_var.copy_(torch.tensor([recent_std**2], device=self.device))

    def normalize(self, reward):
        """å½’ä¸€åŒ–å¥–åŠ±"""
        if len(self.reward_history) < self.warmup_steps:
            return float(reward)

        mean = self.running_mean.item()
        std = torch.sqrt(self.running_var + 1e-8).item()
        normalized = (float(reward) - mean) / std
        return np.clip(normalized, -self.clip_range, self.clip_range)


class ValueNormalization(nn.Module):
    """
    Value Function Normalization

    ç”¨äºç¨³å®šCriticè®­ç»ƒçš„å€¼å‡½æ•°å½’ä¸€åŒ–æŠ€æœ¯
    åŸºäºIsaac Gymå®ç°ï¼Œæä¾›åœ¨çº¿æ›´æ–°å’Œå½’ä¸€åŒ–åŠŸèƒ½
    """
    def __init__(self, beta: float = 0.995, epsilon: float = 1e-8, clip_range: float = 10.0, device: str = None):
        super().__init__()
        self.beta = beta          # æŒ‡æ•°ç§»åŠ¨å¹³å‡ç³»æ•°
        self.epsilon = epsilon      # æ•°å€¼ç¨³å®šæ€§å‚æ•°
        self.clip_range = clip_range # å½’ä¸€åŒ–å€¼è£å‰ªèŒƒå›´

        # æ™ºèƒ½è®¾å¤‡é€‰æ‹© - ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
        self.device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))

        # å¯å­¦ä¹ çš„å‚æ•° - ä½¿ç”¨æ™ºèƒ½è®¾å¤‡é€‰æ‹©
        self.register_buffer('mean', torch.zeros(1, device=self.device))
        self.register_buffer('var', torch.ones(1, device=self.device))
        self.register_buffer('count', torch.zeros(1, device=self.device))

    def update(self, values: torch.Tensor):
        """
        æ›´æ–°å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆåœ¨çº¿EMAæ›´æ–°ï¼‰

        Args:
            values: [batch_size, 1] æˆ– [batch_size] ä»·å€¼å‡½æ•°å€¼
        """
        values = values.view(-1, 1) if values.dim() == 1 else values

        batch_mean = values.mean()
        batch_var = values.var(unbiased=False)
        batch_count = values.numel()

        # åœ¨çº¿æ›´æ–°å‡å€¼å’Œæ–¹å·®
        self.mean = self.beta * self.mean + (1 - self.beta) * batch_mean
        self.var = self.beta * self.var + (1 - self.beta) * batch_var
        self.count += batch_count

    def normalize(self, values: torch.Tensor) -> torch.Tensor:
        """
        å½’ä¸€åŒ–å€¼å‡½æ•°

        Args:
            values: è¾“å…¥å€¼

        Returns:
            normalized_values: å½’ä¸€åŒ–åçš„å€¼
        """
        values = values.view(-1, 1) if values.dim() == 1 else values

        std = torch.sqrt(self.var + self.epsilon)
        normalized = (values - self.mean) / std
        return torch.clamp(normalized, -self.clip_range, self.clip_range).squeeze(-1)

    def denormalize(self, normalized_values: torch.Tensor) -> torch.Tensor:
        """
        åå½’ä¸€åŒ–å€¼å‡½æ•°

        Args:
            normalized_values: å½’ä¸€åŒ–åçš„å€¼

        Returns:
            denormalized_values: åŸå§‹å°ºåº¦çš„å€¼
        """
        normalized_values = normalized_values.view(-1, 1) if normalized_values.dim() == 1 else normalized_values

        std = torch.sqrt(self.var + self.epsilon)
        denormalized = normalized_values * std + self.mean
        return denormalized.squeeze(-1)


class GAE:
    """
    Generalized Advantage Estimation (GAE)

    è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥çš„ç¨³å®šæ–¹æ³•ï¼Œæ”¯æŒè‡ªé€‚åº”æŠ˜æ‰£å› å­
    """
    def __init__(self, gamma: float = 0.99, lam: float = 0.95,
                 device: torch.device = None, use_adaptive_gamma: bool = False,
                 eta_min: float = 0.6, eta_max: float = 0.99):
        self.gamma = gamma              # æŠ˜æ‰£å› å­
        self.lam = lam                  # GAEçš„Î»å‚æ•°
        self.device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))
        self.use_adaptive_gamma = use_adaptive_gamma
        self.eta_min = eta_min
        self.eta_max = eta_max

    def compute_adaptive_gamma(self, action_probs: torch.Tensor) -> torch.Tensor:
        """
        ğŸ¯ æŒ‰ç…§è®ºæ–‡è®¡ç®—è‡ªé€‚åº”æŠ˜æ‰£å› å­ï¼šgamma(s,a;eta) = clip(pi(s,a), eta, 1)

        Args:
            action_probs: [T, N] åŠ¨ä½œæ¦‚ç‡ï¼ˆç­–ç•¥è´¨é‡æŒ‡æ ‡ï¼‰

        Returns:
            adaptive_gamma: [T, N] è‡ªé€‚åº”æŠ˜æ‰£å› å­
        """
        # action_probs æœŸæœ›åœ¨(0,1]ï¼›è¿ç»­åŠ¨ä½œå¯†åº¦å¯èƒ½>1ï¼Œå› æ­¤ä¸Šæ¸¸è¦å…ˆ clamp åˆ° <=1
        action_probs = torch.clamp(action_probs, min=1e-12, max=1.0)
        # âœ… è®ºæ–‡ï¼šgamma(s,a;eta)=clip(pi(s,a), eta, 1)
        return torch.clamp(action_probs, min=self.eta_min, max=1.0)

    def __call__(self, rewards: torch.Tensor, dones: torch.Tensor,
                 values: torch.Tensor, next_values: torch.Tensor,
                 action_probs: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥

        Args:
            rewards: [T, N] å¥–åŠ±åºåˆ—
            dones: [T, N] ç»“æŸæ ‡å¿—
            values: [T, N] ä»·å€¼å‡½æ•°åºåˆ—
            next_values: [T, N] ä¸‹ä¸€çŠ¶æ€ä»·å€¼å‡½æ•°
            action_probs: [T, N] åŠ¨ä½œæ¦‚ç‡ï¼ˆç”¨äºè‡ªé€‚åº”æŠ˜æ‰£å› å­ï¼‰

        Returns:
            advantages: [T, N] ä¼˜åŠ¿å‡½æ•°
            returns: [T, N] å›æŠ¥
        """
        T, N = rewards.shape

         # ğŸ”§ 1) ç»Ÿä¸€æˆ float32
        rewards = rewards.to(self.device).float()
        dones = dones.to(self.device)
        values = values.to(self.device).float()
        next_values = next_values.to(self.device).float()

        # ğŸ”§ 2) æ˜ç¡® advantages / returns ä¹Ÿæ˜¯ float32
        advantages = torch.zeros_like(rewards, dtype=torch.float32)
        returns = torch.zeros_like(rewards, dtype=torch.float32)

        # è®¡ç®—è‡ªé€‚åº”æŠ˜æ‰£å› å­
        if self.use_adaptive_gamma and action_probs is not None:
            action_probs = action_probs.to(self.device)
            gamma_t = self.compute_adaptive_gamma(action_probs)
        else:
            gamma_t = torch.full_like(rewards, self.gamma)

        # ğŸ¯ è®ºæ–‡ä¸€è‡´PFï¼šç®€åŒ–ä½†æ­£ç¡®çš„è¿ä¹˜ç´¯ç§¯å®ç°
        if self.use_adaptive_gamma and action_probs is not None:
            # ğŸ¯ Policy Feedbackæ ¸å¿ƒï¼šè‡ªé€‚åº”æŠ˜æ‰£ä½“ç°ç­–ç•¥è´¨é‡
            # é€šè¿‡æ—¶å˜çš„gamma_t[t]éšå¼å®ç°è¿ä¹˜ç´¯ç§¯æ•ˆåº”

            # ğŸ¯ ä½¿ç”¨ç´¯ç§¯ä¹˜ç§¯è®¡ç®—returnsï¼ˆè®ºæ–‡æ€æƒ³ï¼Œæ›´é«˜æ•ˆå®ç°ï¼‰
            gae = torch.zeros(N, device=self.device)
            cumulative_product = torch.ones(N, device=self.device)  # è¿ä¹˜é¡¹ âˆÎ³

            for t in reversed(range(T)):
                if t == T - 1:
                    next_value = next_values[t]
                else:
                    next_value = values[t + 1]

                # è®¡ç®—TDè¯¯å·®ï¼Œä½¿ç”¨å½“å‰æ­¥çš„è‡ªé€‚åº”æŠ˜æ‰£
                delta = rewards[t] + gamma_t[t] * next_value * (1 - dones[t].float()) - values[t]

                # ğŸ¯ å…³é”®ï¼šç´¯ç§¯ä¹˜ç§¯ä½“ç°è¿ä¹˜æ•ˆåº”
                # cumulative_product ç»´æŠ¤äº† âˆ_{k=t}^{T-1} Î³(s_k,a_k;Î·)
                if t == T - 1:
                    cumulative_product = gamma_t[t]  # æœ€åä¸€æ­¥ï¼šÎ³_T
                else:
                    cumulative_product = gamma_t[t] * cumulative_product  # è¿ä¹˜ï¼šÎ³_t * âˆ_{k=t+1}^{T-1} Î³_k

                # GAEæ›´æ–°ï¼Œä½¿ç”¨ç´¯ç§¯ä¹˜ç§¯å¢å¼ºé•¿æœŸä¾èµ–
                gae = delta + gamma_t[t] * self.lam * (1 - dones[t].float()) * gae

                # ä¿å­˜ç»“æœ
                advantages[t] = gae
                returns[t] = gae + values[t]

        else:
            # æ ‡å‡†GAEè®¡ç®—
            gae = torch.zeros(N, device=self.device)
            for t in reversed(range(T)):
                if t == T - 1:
                    next_value = next_values[t]
                else:
                    next_value = values[t + 1]

                # è®¡ç®—TDè¯¯å·® (ä¿®å¤å¸ƒå°”å¼ é‡å‡æ³•é”™è¯¯)
                delta = rewards[t] + gamma_t[t] * next_value * (1 - dones[t].float()) - values[t]

                # GAEæ›´æ–° (ä¿®å¤å¸ƒå°”å¼ é‡å‡æ³•é”™è¯¯)
                gae = delta + gamma_t[t] * self.lam * (1 - dones[t].float()) * gae

                # ä¿å­˜ç»“æœ
                advantages[t] = gae
                returns[t] = gae + values[t]

        return advantages, returns


def load_config(config_path: str = "config.yaml") -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        config: é…ç½®å­—å…¸
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def set_random_seed(seed: int = 42):
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒå¯å¤ç°æ€§

    Args:
        seed: éšæœºç§å­
    """
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")


def compute_trajectory_smoothness(trajectory: np.ndarray) -> float:
    """
    è®¡ç®—è½¨è¿¹å¹³æ»‘åº¦æŒ‡æ ‡

    Args:
        trajectory: [T, 6] å…³èŠ‚è§’åº¦æˆ–ä½ç½®åºåˆ—

    Returns:
        smoothness: å¹³æ»‘åº¦æŒ‡æ ‡ï¼ˆå€¼è¶Šå°è¶Šå¹³æ»‘ï¼‰
    """
    if len(trajectory) < 3:
        return 0.0

    # è®¡ç®—ä¸€é˜¶å·®åˆ†ï¼ˆé€Ÿåº¦ï¼‰
    velocity = np.diff(trajectory, axis=0)

    # è®¡ç®—äºŒé˜¶å·®åˆ†ï¼ˆåŠ é€Ÿåº¦ï¼‰
    acceleration = np.diff(velocity, axis=0)

    # å¹³æ»‘åº¦æŒ‡æ ‡ï¼šåŠ é€Ÿåº¦çš„L2èŒƒæ•°çš„å¹³å‡å€¼
    if len(acceleration) > 0:
        smoothness = np.mean(np.linalg.norm(acceleration, axis=1))
    else:
        smoothness = 0.0

    return smoothness


def compute_trajectory_metrics(trajectory_data: List[np.ndarray],
                             target_positions: List[np.ndarray],
                             success_threshold: float = 0.005) -> Dict[str, float]:
    """
    è®¡ç®—è½¨è¿¹è´¨é‡æŒ‡æ ‡

    Args:
        trajectory_data: è½¨è¿¹æ•°æ®åˆ—è¡¨
        target_positions: ç›®æ ‡ä½ç½®åˆ—è¡¨
        success_threshold: æˆåŠŸé˜ˆå€¼

    Returns:
        metrics: æŒ‡æ ‡å­—å…¸
    """
    if not trajectory_data:
        return {}

    metrics = {
        'avg_trajectory_length': 0.0,
        'avg_smoothness': 0.0,
        'success_rate': 0.0,
        'avg_final_error': 0.0,
        'trajectory_consistency': 0.0
    }

    # è®¡ç®—è½¨è¿¹é•¿åº¦
    lengths = [len(traj) for traj in trajectory_data]
    metrics['avg_trajectory_length'] = np.mean(lengths)

    # è®¡ç®—å¹³æ»‘åº¦
    smoothness_values = [compute_trajectory_smoothness(traj) for traj in trajectory_data]
    metrics['avg_smoothness'] = np.mean(smoothness_values)

    # è®¡ç®—æˆåŠŸç‡å’Œæœ€ç»ˆè¯¯å·®
    final_errors = []
    successful_count = 0

    for i, (traj, target) in enumerate(zip(trajectory_data, target_positions)):
        if len(traj) > 0:
            # å‡è®¾è½¨è¿¹çš„æœ€åä¸€åˆ—æ˜¯æœ«ç«¯ä½ç½®
            if traj.shape[1] >= 3:
                final_pos = traj[-1, :3]
                final_error = np.linalg.norm(final_pos - target)
                final_errors.append(final_error)

                if final_error < success_threshold:
                    successful_count += 1

    if final_errors:
        metrics['avg_final_error'] = np.mean(final_errors)
        metrics['success_rate'] = successful_count / len(final_errors)

    # è®¡ç®—è½¨è¿¹ä¸€è‡´æ€§ï¼ˆè½¨è¿¹ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼‰
    if len(trajectory_data) > 1:
        # ç®€åŒ–å®ç°ï¼šè®¡ç®—è½¨è¿¹é•¿åº¦çš„æ ‡å‡†å·®
        metrics['trajectory_consistency'] = 1.0 / (1.0 + np.std(lengths))

    return metrics


def save_training_data(episode_data: List[Dict[str, Any]],
                      filepath: str = "csv_output/training_data.csv"):
    """
    ä¿å­˜è®­ç»ƒæ•°æ®åˆ°CSVæ–‡ä»¶

    Args:
        episode_data: è®­ç»ƒæ•°æ®åˆ—è¡¨
        filepath: ä¿å­˜è·¯å¾„
    """
    df = pd.DataFrame(episode_data)
    df.to_csv(filepath, index=False)
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")


def plot_training_curves(training_stats: Dict[str, List],
                        config: Optional[Dict[str, Any]] = None,
                        save_path: str = None,
                        show_plots: bool = False):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿

    Args:
        training_stats: è®­ç»ƒç»Ÿè®¡æ•°æ®
        config: é…ç½®å­—å…¸
        save_path: ä¿å­˜è·¯å¾„
        show_plots: æ˜¯å¦æ˜¾ç¤ºå›¾åƒ
    """
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('UR10e PPO Training Progress', fontsize=16)

    # Episodeå¥–åŠ±
    if 'episode_rewards' in training_stats and training_stats['episode_rewards']:
        axes[0, 0].plot(training_stats['episode_rewards'])
        axes[0, 0].set_title('Episode Rewards')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].grid(True)

        # æ·»åŠ å¹³æ»‘æ›²çº¿
        window_size = min(100, len(training_stats['episode_rewards']))
        if window_size > 0:
            smooth_rewards = pd.Series(training_stats['episode_rewards']).rolling(window=window_size).mean()
            axes[0, 0].plot(smooth_rewards, label=f'Smoothed ({window_size})', alpha=0.7)
            axes[0, 0].legend()

    # Episodeé•¿åº¦
    if 'episode_lengths' in training_stats and training_stats['episode_lengths']:
        axes[0, 1].plot(training_stats['episode_lengths'])
        axes[0, 1].set_title('Episode Lengths')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Steps')
        axes[0, 1].grid(True)

    # ä½ç½®è¯¯å·®
    if 'position_errors' in training_stats and training_stats['position_errors']:
        axes[0, 2].plot(training_stats['position_errors'])
        axes[0, 2].set_title('Position Errors')
        axes[0, 2].set_xlabel('Episode')
        axes[0, 2].set_ylabel('Error (m)')
        axes[0, 2].grid(True)

        # æ·»åŠ æˆåŠŸé˜ˆå€¼çº¿
        success_threshold = config.get('reward', {}).get('accuracy', {}).get('threshold', 0.005) if config else 0.005
        axes[0, 2].axhline(y=success_threshold, color='r', linestyle='--', label='Success Threshold')
        axes[0, 2].legend()

    # æˆåŠŸç‡
    if 'success_rates' in training_stats and training_stats['success_rates']:
        axes[1, 0].plot(training_stats['success_rates'])
        axes[1, 0].set_title('Success Rate (100-episode window)')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Success Rate')
        axes[1, 0].grid(True)
        axes[1, 0].set_ylim([0, 1])

    # è¡°å‡å›åˆç»Ÿè®¡
    if 'decay_stats' in training_stats and training_stats['decay_stats']:
        decay_steps = [stats.get('current_max_steps', 1000) for stats in training_stats['decay_stats']]
        axes[1, 1].plot(decay_steps)
        axes[1, 1].set_title('Decaying Episode Max Steps')
        axes[1, 1].set_xlabel('Episode')
        axes[1, 1].set_ylabel('Max Steps')
        axes[1, 1].grid(True)

    # å¥–åŠ±åˆ†é‡åˆ†æ
    if 'reward_components' in training_stats and training_stats['reward_components']:
        # æå–å¥–åŠ±åˆ†é‡
        accuracy_rewards = []
        smoothness_rewards = []
        energy_rewards = []

        for components in training_stats['reward_components']:
            if isinstance(components, dict):
                accuracy_rewards.append(components.get('accuracy', 0))
                smoothness_rewards.append(components.get('smoothness', 0))
                energy_rewards.append(components.get('energy', 0))

        if accuracy_rewards:
            axes[1, 2].plot(accuracy_rewards, label='Accuracy', alpha=0.7)
        if smoothness_rewards:
            axes[1, 2].plot(smoothness_rewards, label='Smoothness', alpha=0.7)
        if energy_rewards:
            axes[1, 2].plot(energy_rewards, label='Energy', alpha=0.7)

        axes[1, 2].set_title('Reward Components')
        axes[1, 2].set_xlabel('Episode')
        axes[1, 2].set_ylabel('Reward')
        axes[1, 2].grid(True)
        axes[1, 2].legend()

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")

    if show_plots:
        plt.show()
    else:
        plt.close()


def visualize_trajectory(trajectory_data: np.ndarray,
                        target_position: np.ndarray,
                        save_path: str = None,
                        show_plot: bool = False):
    """
    å¯è§†åŒ–å•ä¸ªè½¨è¿¹

    Args:
        trajectory_data: [T, 6] è½¨è¿¹æ•°æ®
        target_position: [3] ç›®æ ‡ä½ç½®
        save_path: ä¿å­˜è·¯å¾„
        show_plot: æ˜¯å¦æ˜¾ç¤ºå›¾åƒ
    """
    fig = plt.figure(figsize=(15, 10))

    # 3Dè½¨è¿¹
    ax1 = fig.add_subplot(2, 3, 1, projection='3d')
    if trajectory_data.shape[1] >= 3:
        ax1.plot(trajectory_data[:, 0], trajectory_data[:, 1], trajectory_data[:, 2], 'b-', label='Trajectory')
        ax1.scatter(target_position[0], target_position[1], target_position[2],
                   c='r', s=100, marker='*', label='Target')
        ax1.scatter(trajectory_data[0, 0], trajectory_data[0, 1], trajectory_data[0, 2],
                   c='g', s=50, marker='o', label='Start')
        ax1.scatter(trajectory_data[-1, 0], trajectory_data[-1, 1], trajectory_data[-1, 2],
                   c='orange', s=50, marker='s', label='End')
    ax1.set_xlabel('X (m)')
    ax1.set_ylabel('Y (m)')
    ax1.set_zlabel('Z (m)')
    ax1.set_title('3D Trajectory')
    ax1.legend()

    # å…³èŠ‚è§’åº¦
    for i in range(6):
        ax2 = fig.add_subplot(2, 3, i + 2)
        ax2.plot(trajectory_data[:, i])
        ax2.set_title(f'Joint {i + 1} Angle')
        ax2.set_xlabel('Time Step')
        ax2.set_ylabel('Angle (rad)')
        ax2.grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ è½¨è¿¹å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")

    if show_plot:
        plt.show()
    else:
        plt.close()


def create_experiment_directory(base_dir: str = "./experiments") -> str:
    """
    åˆ›å»ºå®éªŒç›®å½•

    Args:
        base_dir: åŸºç¡€ç›®å½•

    Returns:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = os.path.join(base_dir, f"ur10e_ppo_{timestamp}")

    # åˆ›å»ºå­ç›®å½•
    subdirs = [
        "checkpoints",      # æ¨¡å‹æ£€æŸ¥ç‚¹
        "logs",            # æ—¥å¿—æ–‡ä»¶
        "csv_output",      # CSVæ•°æ®
        "plots",           # è®­ç»ƒæ›²çº¿
        "trajectories",    # è½¨è¿¹å¯è§†åŒ–
        "config",          # é…ç½®æ–‡ä»¶
        "models"           # æœ€ç»ˆæ¨¡å‹
    ]

    for subdir in subdirs:
        os.makedirs(os.path.join(experiment_dir, subdir), exist_ok=True)

    print(f"ğŸ“ å®éªŒç›®å½•å·²åˆ›å»º: {experiment_dir}")
    return experiment_dir


def save_experiment_config(config: Dict[str, Any], experiment_dir: str):
    """
    ä¿å­˜å®éªŒé…ç½®

    Args:
        config: é…ç½®å­—å…¸
        experiment_dir: å®éªŒç›®å½•
    """
    config_path = os.path.join(experiment_dir, "config", "config.yaml")
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

    # åŒæ—¶ä¿å­˜ä¸ºJSONæ ¼å¼
    json_path = os.path.join(experiment_dir, "config", "config.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    print(f"âš™ï¸  å®éªŒé…ç½®å·²ä¿å­˜")


def compute_success_metrics(position_errors: List[float],
                           threshold: float = 0.005) -> Dict[str, float]:
    """
    è®¡ç®—æˆåŠŸæŒ‡æ ‡

    Args:
        position_errors: ä½ç½®è¯¯å·®åˆ—è¡¨
        threshold: æˆåŠŸé˜ˆå€¼

    Returns:
        metrics: æˆåŠŸæŒ‡æ ‡å­—å…¸
    """
    if not position_errors:
        return {}

    success_count = sum(1 for error in position_errors if error < threshold)
    total_count = len(position_errors)

    metrics = {
        'success_rate': success_count / total_count,
        'total_episodes': total_count,
        'successful_episodes': success_count,
        'mean_error': np.mean(position_errors),
        'std_error': np.std(position_errors),
        'median_error': np.median(position_errors),
        'min_error': np.min(position_errors),
        'max_error': np.max(position_errors)
    }

    return metrics


def generate_training_report(training_stats: Dict[str, List],
                           config: Dict[str, Any],
                           experiment_dir: str):
    """
    ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š

    Args:
        training_stats: è®­ç»ƒç»Ÿè®¡æ•°æ®
        config: é…ç½®å­—å…¸
        experiment_dir: å®éªŒç›®å½•
    """
    report_path = os.path.join(experiment_dir, "training_report.txt")

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("UR10e PPO å¤šç›®æ ‡æœ€ä¼˜è½¨è¿¹è§„åˆ’è®­ç»ƒæŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")

        # é…ç½®ä¿¡æ¯
        f.write("ğŸ“‹ é…ç½®ä¿¡æ¯:\n")
        f.write(f"æœ€å¤§è®­ç»ƒè½®æ•°: {config['train']['max_episodes']}\n")
        f.write(f"çŠ¶æ€ç©ºé—´: 25ç»´\n")
        f.write(f"åŠ¨ä½œç©ºé—´: 6ç»´\n")
        f.write(f"æœ€å¤§æ­¥æ•°: {config['env']['max_steps']}\n")
        f.write(f"è¡°å‡å›åˆæœºåˆ¶: {'å¯ç”¨' if config['decay_episode']['enabled'] else 'ç¦ç”¨'}\n")
        f.write(f"æˆåŠŸç‡é˜ˆå€¼: {config['decay_episode']['success_threshold']}\n\n")

        # è®­ç»ƒç»Ÿè®¡
        if 'episode_rewards' in training_stats and training_stats['episode_rewards']:
            f.write("ğŸ“Š è®­ç»ƒç»Ÿè®¡:\n")
            f.write(f"æ€»è®­ç»ƒè½®æ•°: {len(training_stats['episode_rewards'])}\n")
            f.write(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(training_stats['episode_rewards'][-100:]):.2f}\n")
            f.write(f"æœ€ç»ˆæˆåŠŸç‡: {training_stats['success_rates'][-1]:.2%}\n")
            f.write(f"æœ€ç»ˆå¹³å‡ä½ç½®è¯¯å·®: {np.mean(training_stats['position_errors'][-100:]):.4f}m\n")

            # æˆåŠŸæŒ‡æ ‡
            if 'position_errors' in training_stats:
                success_metrics = compute_success_metrics(training_stats['position_errors'])
                f.write(f"\nğŸ¯ æˆåŠŸæŒ‡æ ‡:\n")
                for key, value in success_metrics.items():
                    if key == 'success_rate':
                        f.write(f"{key}: {value:.2%}\n")
                    else:
                        f.write(f"{key}: {value:.4f}\n")

    print(f"ğŸ“ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


class RewardNormalizer:
    """
    å¥–åŠ±å½’ä¸€åŒ–å™¨

    ç”¨äºç¨³å®šPPOè®­ç»ƒçš„å¥–åŠ±å½’ä¸€åŒ–æŠ€æœ¯ï¼Œæ”¯æŒåœ¨çº¿æ›´æ–°å’Œå¤šç§å½’ä¸€åŒ–ç­–ç•¥
    """

    def __init__(self,
                 gamma: float = 0.99,
                 clip_range: float = 5.0,
                 epsilon: float = 1e-8,
                 normalize_method: str = 'running_stats',
                 warmup_steps: int = 100,
                 history_size: int = 10000):
        """
        åˆå§‹åŒ–å¥–åŠ±å½’ä¸€åŒ–å™¨

        Args:
            gamma: æŠ˜æ‰£å› å­ï¼Œç”¨äºè®¡ç®—æŠ˜æ‰£å¥–åŠ±ç»Ÿè®¡
            clip_range: å½’ä¸€åŒ–å€¼è£å‰ªèŒƒå›´
            epsilon: æ•°å€¼ç¨³å®šæ€§å‚æ•°
            normalize_method: å½’ä¸€åŒ–æ–¹æ³• ['running_stats', 'batch_stats', 'rank']
            warmup_steps: é¢„çƒ­æ­¥æ•°ï¼ŒåˆæœŸä¸è¿›è¡Œå½’ä¸€åŒ–
            history_size: å¥–åŠ±å†å²è®°å½•å¤§å°
        """
        self.gamma = gamma
        self.clip_range = clip_range
        self.epsilon = epsilon
        self.normalize_method = normalize_method
        self.warmup_steps = warmup_steps
        self.history_size = history_size

        # è¿è¡Œæ—¶ç»Ÿè®¡é‡
        self.running_mean = 0.0
        self.running_var = 1.0
        self.running_count = 0
        self.beta = 0.99  # æŒ‡æ•°ç§»åŠ¨å¹³å‡ç³»æ•°

        # å¥–åŠ±å†å²
        self.reward_history = []
        self.discounted_reward_history = []

        # æ‰¹æ¬¡ç»Ÿè®¡
        self.batch_rewards = []

    def update(self, reward: float, done: bool = False):
        """
        æ›´æ–°å½’ä¸€åŒ–å™¨ç»Ÿè®¡é‡

        Args:
            reward: å½“å‰å¥–åŠ±å€¼
            done: æ˜¯å¦å›åˆç»“æŸ
        """
        self.reward_history.append(reward)
        self.running_count += 1

        # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°
        self.running_mean = self.beta * self.running_mean + (1 - self.beta) * reward
        delta = reward - self.running_mean
        self.running_var = self.beta * self.running_var + (1 - self.beta) * delta * delta

        # ç»´æŠ¤å†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.reward_history) > self.history_size:
            self.reward_history = self.reward_history[-self.history_size//2:]

        # å›åˆç»“æŸæ—¶è®¡ç®—æŠ˜æ‰£å¥–åŠ±ç»Ÿè®¡
        if done and len(self.reward_history) > 1:
            self._update_discounted_stats()

    def _update_discounted_stats(self):
        """æ›´æ–°æŠ˜æ‰£å¥–åŠ±ç»Ÿè®¡"""
        if not self.reward_history:
            return

        # è®¡ç®—æœ€è¿‘ä¸€ä¸ªepisodeçš„æŠ˜æ‰£å¥–åŠ±
        discounted_rewards = []
        reward_sum = 0.0
        for reward in reversed(self.reward_history):
            reward_sum = reward + self.gamma * reward_sum
            discounted_rewards.append(reward_sum)

        discounted_rewards.reverse()
        self.discounted_reward_history.extend(discounted_rewards)

        # ç»´æŠ¤æŠ˜æ‰£å¥–åŠ±å†å²
        if len(self.discounted_reward_history) > self.history_size:
            self.discounted_reward_history = self.discounted_reward_history[-self.history_size//2:]

    def normalize(self, reward: float) -> float:
        """
        å½’ä¸€åŒ–å•ä¸ªå¥–åŠ±

        Args:
            reward: åŸå§‹å¥–åŠ±å€¼

        Returns:
            normalized_reward: å½’ä¸€åŒ–åçš„å¥–åŠ±å€¼
        """
        if self.running_count < self.warmup_steps:
            return reward  # é¢„çƒ­æœŸä¸å½’ä¸€åŒ–

        if self.normalize_method == 'running_stats':
            return self._normalize_running_stats(reward)
        elif self.normalize_method == 'batch_stats':
            return self._normalize_batch_stats(reward)
        elif self.normalize_method == 'rank':
            return self._normalize_rank(reward)
        else:
            return reward

    def _normalize_running_stats(self, reward: float) -> float:
        """ä½¿ç”¨è¿è¡Œç»Ÿè®¡é‡å½’ä¸€åŒ–"""
        std = np.sqrt(self.running_var + self.epsilon)
        normalized = (reward - self.running_mean) / std
        return np.clip(normalized, -self.clip_range, self.clip_range)

    def _normalize_batch_stats(self, reward: float) -> float:
        """ä½¿ç”¨æ‰¹æ¬¡ç»Ÿè®¡é‡å½’ä¸€åŒ–"""
        if len(self.reward_history) < 10:
            return reward

        # ä½¿ç”¨æœ€è¿‘çš„å¥–åŠ±ä½œä¸ºæ‰¹æ¬¡
        recent_rewards = self.reward_history[-min(100, len(self.reward_history)):]
        batch_mean = np.mean(recent_rewards)
        batch_std = np.std(recent_rewards) + self.epsilon

        normalized = (reward - batch_mean) / batch_std
        return np.clip(normalized, -self.clip_range, self.clip_range)

    def _normalize_rank(self, reward: float) -> float:
        """ä½¿ç”¨ç§©å½’ä¸€åŒ–ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰"""
        if len(self.reward_history) < 10:
            return reward

        # è®¡ç®—å½“å‰å¥–åŠ±åœ¨å†å²ä¸­çš„ç™¾åˆ†ä½
        count_smaller = sum(1 for r in self.reward_history if r < reward)
        percentile = count_smaller / len(self.reward_history)

        # æ˜ å°„åˆ°[-1, 1]èŒƒå›´
        normalized = 2 * percentile - 1
        return np.clip(normalized, -self.clip_range, self.clip_range)

    def normalize_batch(self, rewards: np.ndarray) -> np.ndarray:
        """
        æ‰¹é‡å½’ä¸€åŒ–å¥–åŠ±

        Args:
            rewards: [batch_size] å¥–åŠ±æ•°ç»„

        Returns:
            normalized_rewards: å½’ä¸€åŒ–åçš„å¥–åŠ±æ•°ç»„
        """
        if self.running_count < self.warmup_steps:
            return rewards

        normalized_rewards = np.array([self.normalize(r) for r in rewards])
        return normalized_rewards

    def get_stats(self) -> dict:
        """è·å–å½’ä¸€åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'method': self.normalize_method,
            'running_mean': self.running_mean,
            'running_var': self.running_var,
            'running_std': np.sqrt(self.running_var + self.epsilon),
            'count': self.running_count,
            'recent_mean': np.mean(self.reward_history[-100:]) if self.reward_history else 0.0,
            'recent_std': np.std(self.reward_history[-100:]) if len(self.reward_history) > 1 else 0.0,
            'history_size': len(self.reward_history),
            'warmup_progress': min(1.0, self.running_count / self.warmup_steps)
        }

    def reset(self):
        """é‡ç½®å½’ä¸€åŒ–å™¨ï¼ˆä¿ç•™å­¦ä¹ åˆ°çš„ç»Ÿè®¡é‡ï¼‰"""
        self.reward_history = []
        self.batch_rewards = []

    def full_reset(self):
        """å®Œå…¨é‡ç½®å½’ä¸€åŒ–å™¨"""
        self.reward_history = []
        self.discounted_reward_history = []
        self.batch_rewards = []
        self.running_mean = 0.0
        self.running_var = 1.0
        self.running_count = 0


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""

    def __init__(self, log_dir: str = "./logs"):
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)

        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = os.path.join(log_dir, f"training_{timestamp}.log")

    def log(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] [{level}] {message}"

        # å†™å…¥æ–‡ä»¶
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')

        # æ‰“å°åˆ°æ§åˆ¶å°
        print(log_message)

    def log_experiment_start(self, config: Dict[str, Any]):
        """è®°å½•å®éªŒå¼€å§‹"""
        self.log("ğŸš€ å¼€å§‹UR10e PPOè®­ç»ƒå®éªŒ")
        self.log(f"ğŸ“‹ é…ç½®: {config}")
        self.log(f"ğŸ¯ çŠ¶æ€ç©ºé—´: 25ç»´, åŠ¨ä½œç©ºé—´: 6ç»´")
        self.log(f"ğŸ”§ è¡°å‡å›åˆæœºåˆ¶: {'å¯ç”¨' if config['decay_episode']['enabled'] else 'ç¦ç”¨'}")

    def log_experiment_end(self, final_stats: Dict[str, Any]):
        """è®°å½•å®éªŒç»“æŸ"""
        self.log("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        self.log(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {final_stats}")


def validate_config(config: Dict[str, Any]) -> bool:
    """
    éªŒè¯é…ç½®æ–‡ä»¶çš„æœ‰æ•ˆæ€§

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        is_valid: é…ç½®æ˜¯å¦æœ‰æ•ˆ
    """
    try:
        # æ£€æŸ¥å¿…éœ€çš„é…ç½®é¡¹
        required_sections = ['env', 'ppo', 'train', 'reward']
        for section in required_sections:
            if section not in config:
                print(f"âŒ é…ç½®ç¼ºå°‘å¿…éœ€éƒ¨åˆ†: {section}")
                return False

        # æ£€æŸ¥ç¯å¢ƒé…ç½®
        if 'xml_path' not in config['env']:
            print("âŒ ç¼ºå°‘XMLæ¨¡å‹è·¯å¾„")
            return False

        # æ£€æŸ¥PPOé…ç½®
        ppo_required = ['lr_actor', 'lr_critic', 'clip_eps', 'gamma']
        for key in ppo_required:
            if key not in config['ppo']:
                print(f"âŒ PPOé…ç½®ç¼ºå°‘å¿…éœ€é¡¹: {key}")
                return False

        # æ£€æŸ¥å¥–åŠ±é…ç½®
        reward_required = ['accuracy', 'smoothness', 'energy']
        for key in reward_required:
            if key not in config['reward']:
                print(f"âŒ å¥–åŠ±é…ç½®ç¼ºå°‘å¿…éœ€é¡¹: {key}")
                return False

        print("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")


def assert_same_device(*tensors, device=None):
    """
    ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š

    Args:
        *tensors: è¦æ£€æŸ¥çš„å¼ é‡åˆ—è¡¨
        device: æœŸæœ›çš„è®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªå¼ é‡çš„è®¾å¤‡

    Raises:
        AssertionError: å¦‚æœå‘ç°å¼ é‡åœ¨ä¸åŒè®¾å¤‡ä¸Š
    """
    if not tensors:
        return

    # å¦‚æœæŒ‡å®šäº†è®¾å¤‡ï¼Œæ£€æŸ¥æ‰€æœ‰å¼ é‡æ˜¯å¦åœ¨è¯¥è®¾å¤‡ä¸Š
    if device is not None:
        for i, tensor in enumerate(tensors):
            if tensor.device != device:
                raise AssertionError(
                    f"å¼ é‡ {i} åœ¨è®¾å¤‡ {tensor.device}ï¼ŒæœŸæœ›åœ¨ {device}"
                )
        return

    # å¦åˆ™æ£€æŸ¥æ‰€æœ‰å¼ é‡æ˜¯å¦åœ¨åŒä¸€è®¾å¤‡ä¸Š
    first_device = tensors[0].device
    for i, tensor in enumerate(tensors):
        if tensor.device != first_device:
            raise AssertionError(
                f"è®¾å¤‡ä¸åŒ¹é…: å¼ é‡ 0 åœ¨ {first_device}ï¼Œå¼ é‡ {i} åœ¨ {tensor.device}"
            )


def check_tensor_devices(tensor_dict: dict, name: str = "Tensor Dict"):
    """
    æ£€æŸ¥å­—å…¸ä¸­æ‰€æœ‰å¼ é‡çš„è®¾å¤‡ä¸€è‡´æ€§

    Args:
        tensor_dict: åŒ…å«å¼ é‡çš„å­—å…¸
        name: å­—å…¸åç§°ï¼Œç”¨äºé”™è¯¯ä¿¡æ¯

    Returns:
        bool: å¦‚æœæ‰€æœ‰å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Šè¿”å›True
    """
    devices = {}
    for key, tensor in tensor_dict.items():
        if hasattr(tensor, 'device'):
            if tensor.device not in devices:
                devices[tensor.device] = []
            devices[tensor.device].append(key)

    if len(devices) > 1:
        print(f"âš ï¸ {name} ä¸­çš„è®¾å¤‡ä¸ä¸€è‡´:")
        for device, keys in devices.items():
            print(f"   {device}: {keys}")
        return False
    return True


def get_tensor_device(tensor, default_device=None):
    """
    å®‰å…¨è·å–å¼ é‡è®¾å¤‡

    Args:
        tensor: è¾“å…¥å¼ é‡æˆ–æ•°ç»„
        default_device: é»˜è®¤è®¾å¤‡ï¼ˆå¦‚æœè¾“å…¥ä¸æ˜¯å¼ é‡ï¼‰

    Returns:
        torch.device: å¼ é‡è®¾å¤‡
    """
    if hasattr(tensor, 'device'):
        return tensor.device
    elif default_device is not None:
        return default_device
    else:
        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def ensure_device(tensor, device):
    """
    ç¡®ä¿å¼ é‡åœ¨æŒ‡å®šè®¾å¤‡ä¸Š

    Args:
        tensor: è¾“å…¥å¼ é‡
        device: ç›®æ ‡è®¾å¤‡

    Returns:
        torch.Tensor: åœ¨æŒ‡å®šè®¾å¤‡ä¸Šçš„å¼ é‡
    """
    if hasattr(tensor, 'to'):
        return tensor.to(device)
    else:
        # å¦‚æœä¸æ˜¯å¼ é‡ï¼ˆå¦‚numpyæ•°ç»„ï¼‰ï¼Œè½¬æ¢ä¸ºå¼ é‡
        return torch.tensor(tensor, device=device)


def _device_consistency_check():
    """
    è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥ - ä¿®å¤æœåŠ¡å™¨è®¾å¤‡ä¸åŒ¹é…é—®é¢˜

    ä¸“é—¨é’ˆå¯¹æœåŠ¡å™¨ç¯å¢ƒä¸­cuda:0å’Œcuda:2è®¾å¤‡ä¸åŒ¹é…çš„è§£å†³æ–¹æ¡ˆ
    """
    # ğŸ”§ Phase 1: å¼ºåˆ¶ç¯å¢ƒå˜é‡è®¾ç½®ï¼ˆä¿®å¤æœåŠ¡å™¨è®¾å¤‡ä¸åŒ¹é…ï¼‰
    print("ğŸ”§ [SERVER FIX] å¼ºåˆ¶CUDAè®¾å¤‡ä¸€è‡´æ€§è®¾ç½®...")

    # ğŸ¯ **ç”¨æˆ·æœåŠ¡å™¨ä½¿ç”¨GPU 2ï¼Œå¼ºåˆ¶è®¾ç½®GPU 2**
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # **ç”¨æˆ·æœåŠ¡å™¨ä½¿ç”¨GPU 2**
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

    # æ£€æŸ¥CUDAç¯å¢ƒ
    if torch.cuda.is_available():
        print(f"   âœ… CUDAå¯ç”¨ï¼Œç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"   âœ… æ£€æµ‹åˆ°GPUæ•°é‡: {torch.cuda.device_count()}")

        # ğŸ¯ **ç”¨æˆ·æœåŠ¡å™¨å¼ºåˆ¶ä½¿ç”¨GPU 2**
        target_device_index = 0  # è®¾ç½®CUDA_VISIBLE_DEVICES=2åï¼ŒGPU 2å˜ä¸ºç´¢å¼•0
        target_device = 'cuda:0'  # åœ¨å¯è§è®¾å¤‡ä¸­ï¼ŒGPU 2ç°åœ¨æ˜¯cuda:0
        try:
            torch.cuda.set_device(target_device_index)  # å¼ºåˆ¶è®¾ç½®ä¸ºGPU 2ï¼ˆç°åœ¨æ˜¯ç´¢å¼•0ï¼‰
            current_device = torch.cuda.current_device()
            print(f"   ğŸ”’ [FORCED] å½“å‰CUDAè®¾å¤‡: GPU {current_device} (åŸGPU 2)")

            # éªŒè¯è®¾å¤‡ç¡®å®å¯ç”¨
            if current_device == target_device_index:
                print(f"   âœ… [SUCCESS] æˆåŠŸå¼ºåˆ¶ä½¿ç”¨GPU 2 (ç´¢å¼•{current_device})")
            else:
                print(f"   âš ï¸  [WARNING] æœŸæœ›GPU 2(ç´¢å¼•0)ï¼Œå®é™…GPU {current_device}")

        except Exception as e:
            print(f"   âŒ [ERROR] å¼ºåˆ¶è®¾å¤‡è®¾ç½®å¤±è´¥: {e}")
            print(f"   ğŸ”„ [FALLBACK] ä½¿ç”¨CPUæ¨¡å¼")
            return torch.device('cpu')
    else:
        print("   âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return torch.device('cpu')

    # ğŸ¯ [CRITICAL] æœåŠ¡å™¨è®¾å¤‡ä¸€è‡´æ€§éªŒè¯
    print("ğŸ” [SERVER DIAG] æœåŠ¡å™¨è®¾å¤‡ä¸€è‡´æ€§è¯Šæ–­:")

    # æµ‹è¯•å¼ é‡åˆ›å»ºå’Œè®¾å¤‡æ£€æŸ¥
    try:
        test_tensor = torch.randn(10, 10, device='cuda:0')  # è¿™æ˜¯åŸGPU 2
        actual_device = test_tensor.device
        print(f"   ğŸ§ª æµ‹è¯•å¼ é‡è®¾å¤‡: {actual_device} (åŸGPU 2)")

        # æ£€æŸ¥æ‰€æœ‰å¯è§GPUï¼ˆç°åœ¨åªæœ‰GPU 2å¯è§ï¼‰
        for i in range(torch.cuda.device_count()):
            device_name = torch.cuda.get_device_name(i)
            device_props = torch.cuda.get_device_properties(i)
            print(f"   GPU {i} (åŸGPU 2): {device_name} (å†…å­˜: {device_props.total_memory/1024**3:.1f}GB)")

        # ç¡®ä¿æ‰€æœ‰åç»­æ“ä½œéƒ½ä½¿ç”¨cuda:0ï¼ˆåŸGPU 2ï¼‰
        if str(actual_device) == 'cuda:0':
            print(f"   âœ… [DEVICE OK] ä½¿ç”¨ç›®æ ‡è®¾å¤‡: {actual_device} (åŸGPU 2)")
            return torch.device('cuda:0')
        else:
            print(f"   âŒ [DEVICE MISMATCH] æœŸæœ›cuda:0(åŸGPU 2)ï¼Œå®é™…{actual_device}")
            print(f"   ğŸ”„ [FALLBACK] å¼ºåˆ¶è¿”å›cuda:0")
            return torch.device('cuda:0')

    except Exception as e:
        print(f"   âŒ [CRITICAL ERROR] è®¾å¤‡æµ‹è¯•å¤±è´¥: {e}")
        print(f"   ğŸ”„ [FALLBACK] ä½¿ç”¨CPUæ¨¡å¼")
        return torch.device('cpu')


def diagnose_server_environment():
    """
    æœåŠ¡å™¨ç¯å¢ƒå…¨é¢è¯Šæ–­
    ä¸“é—¨ç”¨äºè¯Šæ–­ä¸ºä»€ä¹ˆæœ¬åœ°æ­£å¸¸ä½†æœåŠ¡å™¨å¤±è´¥çš„é—®é¢˜
    """
    print("=" * 80)
    print("ğŸ¥ [SERVER DIAGNOSIS] æœåŠ¡å™¨ç¯å¢ƒå…¨é¢è¯Šæ–­")
    print("=" * 80)

    # 1. Pythonå’Œç¯å¢ƒæ£€æŸ¥
    print("\nğŸ Pythonç¯å¢ƒ:")
    import sys
    print(f"   Pythonç‰ˆæœ¬: {sys.version}")
    print(f"   å¯æ‰§è¡Œæ–‡ä»¶: {sys.executable}")

    # 2. CUDAç¯å¢ƒè¯¦ç»†æ£€æŸ¥
    print("\nğŸ”¥ CUDAç¯å¢ƒ:")
    print(f"   PyTorch CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"   PyTorch CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   ç¼–è¯‘çš„CUDAç‰ˆæœ¬: {torch.version.cuda or 'N/A'}")

    if torch.cuda.is_available():
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"      è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"      æ€»å†…å­˜: {props.total_memory / 1024**3:.1f} GB")
            print(f"      å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}")

    # 3. ç¯å¢ƒå˜é‡æ£€æŸ¥
    print("\nğŸŒ ç¯å¢ƒå˜é‡:")
    import os
    cuda_vars = ['CUDA_VISIBLE_DEVICES', 'PYTORCH_CUDA_ALLOC_CONF',
                 'CUDA_DEVICE_ORDER', 'CUDA_LAUNCH_BLOCKING']
    for var in cuda_vars:
        value = os.environ.get(var, 'Not set')
        print(f"   {var}: {value}")

    # 4. å½“å‰è®¾å¤‡çŠ¶æ€
    print("\nğŸ“ å½“å‰è®¾å¤‡çŠ¶æ€:")
    if torch.cuda.is_available():
        current = torch.cuda.current_device()
        print(f"   å½“å‰è®¾å¤‡: GPU {current}")
        print(f"   å½“å‰è®¾å¤‡å: {torch.cuda.get_device_name(current)}")

        # å†…å­˜çŠ¶æ€
        allocated = torch.cuda.memory_allocated(current)
        reserved = torch.cuda.memory_reserved(current)
        print(f"   å·²åˆ†é…å†…å­˜: {allocated/1024**2:.1f} MB")
        print(f"   å·²é¢„ç•™å†…å­˜: {reserved/1024**2:.1f} MB")

    # 5. Isaac Gymç¯å¢ƒæ£€æŸ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    print("\nğŸ® Isaac Gymç¯å¢ƒ:")
    try:
        import gym
        print(f"   Isaac Gymå¯ç”¨: True")
        print(f"   è·¯å¾„: {gym.__file__ if hasattr(gym, '__file__') else 'Built-in'}")
    except ImportError:
        print(f"   Isaac Gymå¯ç”¨: False")

    # 6. æ¨èä¿®å¤æªæ–½
    print("\nğŸ’¡ æ¨èä¿®å¤æªæ–½:")
    if torch.cuda.is_available() and torch.cuda.device_count() > 1:
        print("   1. å¤šGPUç¯å¢ƒæ£€æµ‹åˆ°ï¼Œå¼ºåˆ¶ä½¿ç”¨GPU 2:")
        print("      export CUDA_VISIBLE_DEVICES=2")
        print("      export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128")
        print("   2. åœ¨ä»£ç ä¸­å¼ºåˆ¶è®¾å¤‡æ£€æŸ¥")
        print("   3. ç›‘æ§ç¬¬500æ­¥é™„è¿‘çš„è®¾å¤‡åˆ‡æ¢")
    elif not torch.cuda.is_available():
        print("   1. CUDAä¸å¯ç”¨ï¼Œæ£€æŸ¥NVIDIAé©±åŠ¨:")
        print("      nvidia-smi")
        print("      æ£€æŸ¥PyTorch CUDAç‰ˆæœ¬åŒ¹é…")
    else:
        print("   1. ç¯å¢ƒçœ‹èµ·æ¥æ­£å¸¸ï¼Œæ£€æŸ¥ä»£ç ä¸­çš„è®¾å¤‡ä¸€è‡´æ€§")

    print("=" * 80)


def get_forced_device():
    """
    è·å–å¼ºåˆ¶ç»Ÿä¸€çš„è®¾å¤‡ï¼Œè§£å†³æœåŠ¡å™¨è®¾å¤‡ä¸åŒ¹é…é—®é¢˜

    Returns:
        torch.device: å¼ºåˆ¶ç»Ÿä¸€çš„è®¾å¤‡ï¼ˆä¼˜å…ˆcuda:0ï¼Œå¦åˆ™cpuï¼‰
    """
    # é¦–å…ˆè¿è¡Œè®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥
    device = _device_consistency_check()

    # å¦‚æœæ˜¯æœåŠ¡å™¨ç¯å¢ƒä¸”å‡ºç°é—®é¢˜ï¼Œè¿è¡Œå…¨é¢è¯Šæ–­
    if torch.cuda.is_available() and torch.cuda.device_count() > 1:
        print("ğŸš¨ [SERVER WARNING] æ£€æµ‹åˆ°å¤šGPUç¯å¢ƒï¼Œå¯ç”¨æœåŠ¡å™¨ä¿®å¤æ¨¡å¼")
        diagnose_server_environment()

    return device