# Isaac Gym PPO配置文件
# UR10e RL-PID混合控制 - Isaac Gym版本（清理重复参数）

# 环境配置
env:
  num_envs: 1                    # 并行环境数量
  max_steps: 256                 # 增加最大步数，给足够时间到达目标
  device_id: 0                   # GPU设备ID (用户服务器使用GPU 2)
  dt: 0.01                       # 100Hz，与MuJoCo原始配置一致

  # 🎯 目标位置范围 (UR10e舒适工作区)
  # 缩小范围确保目标可达，避免过于困难的远距离目标
  target_range:
    x: [-0.6, 0.6]               # X轴范围 (缩小到舒适工作区)
    y: [-0.6, 0.6]               # Y轴范围 (缩小到舒适工作区)
    z: [0.1, 0.8]                # Z轴范围 (避免过高或过低的目标)

# 🎯 设备配置（参考isaac_gym_manipulator方案）
device: "cuda:0"                 # PyTorch设备 (用户服务器使用GPU 2)
sim:
  device_id: 0                  # Isaac Gym仿真设备ID (用户服务器使用GPU 2)
  # 物理引擎配置（基于isaac_gym_manipulator成功配置）
  solver_type: 1                 # 使用solver_type=1
  num_position_iterations: 4     # 位置迭代次数
  num_velocity_iterations: 1     # 速度迭代次数
  num_threads: 0                 # 线程数（0=自动）
  use_gpu: true                  # 启用GPU
  use_gpu_pipeline: false        # 禁用GPU渲染管线

  # 时间步长配置
  substeps: 2                    # 使用2个子步骤，提高仿真稳定性

graphics:
  graphics_device_id: 0          # 图形设备ID (用户服务器使用GPU 2)

visualization:
  enable: true                   # 启用可视化（参考isaac_gym_manipulator）
  render_freq: 1                 # 渲染频率

# PPO算法配置（优化超参数）
ppo:
  # 学习率（降低以稳定训练）
  lr_actor: 5e-4                 # 降低Actor学习率，更稳定学习
  lr_critic: 5e-4                # 降低Critic学习率

  # PPO核心参数（保守设置）
  clip_eps: 0.15                 # 降低裁剪参数，更保守更新
  gamma: 0.995                   # 略微提高折扣因子，更重视长期奖励
  lam: 0.95                      # GAE λ参数保持不变

  # 正则化（增加探索）
  entropy_coef: 0.01             # 增加熵系数，鼓励更多探索
  value_coef: 0.5                # 价值函数系数保持不变

  # 梯度处理
  max_grad_norm: 0.5             # 梯度裁剪阈值保持不变

# 训���配置（优化rollout策略）
train:
  rollout_length: 512           # 降低rollout长度，更适合500步episode
  batch_size: 64                 # 批次大小
  num_updates: 10                # 每次收集后更新次数
  num_episodes: 10000            # 总训练回合数

  # 保存和日志
  save_interval: 100             # 保存间隔
  log_interval: 10               # 日志间隔

# 🎯 新的奖励函数配置（基于关节角度误差）
reward:
  # 🎯 关节角度误差奖励（主要奖励）
  accuracy:
    weight: 2.0                  # 关节角度误差权重
    threshold: 0.05              # 末端位置成功阈值（次要）

  # 🏃 关节进度奖励（奖励误差减少）
  progress:
    weight: 1.0                  # 关节角度误差减少权重

  # 📍 末端位置误差奖励（次要奖励）
  position:
    weight: 0.5                  # 末端位置误差权重

  # 🔧 PID参数稳定性奖励
  stability:
    weight: 0.1                  # PID参数合理性约束权重

  # 🎊 成功奖励
  extra:
    success_reward: 5.0          # 关节到达成功奖励
    position_success_reward: 2.0  # 末端位置成功奖励

# UR10e PID参数配置（基于DDPG成功案例调优）
# 参考DDPG中的温和参数，避免过大的增益导致震荡
pid_params:
  # 基础PID参数（调优后的温和值）
  base_gains:
    # 比例增益 P: [shoulder_pan, shoulder_lift, elbow, wrist_1, wrist_2, wrist_3]
    p: [2000.0, 1500.0, 1200.0, 300.0, 300.0, 200.0]  # DDPG成功案例的值
    # 微分增益 D: [shoulder_pan, shoulder_lift, elbow, wrist_1, wrist_2, wrist_3]
    d: [80.0, 50.0, 40.0, 15.0, 15.0, 10.0]        # 统一的阻尼系数
    # 积分增益 I: [shoulder_pan, shoulder_lift, elbow, wrist_1, wrist_2, wrist_3]
    i: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                   # 暂时不用积分项

  # 力矩限制（保守值，确保安全）
  torque_limits: [264.0, 264.0, 120.0, 43.2, 43.2, 43.2]  # 80% of官方限制

# 网络架构配置
network:
  # Actor网络
  actor:
    hidden_dim: 64               # 隐藏层维度
    num_layers: 3                # 网络层数
    activation: "relu"           # 激活函数

  # Critic网络
  critic:
    hidden_dim: 64               # 隐藏层维度
    num_layers: 3                # 网络层数
    activation: "relu"           # 激活函数

# 价值归一化配置
value_normalization:
  enabled: true                  # 启用价值归一化
  beta: 0.995                    # EMA系数
  epsilon: 1e-5                  # 数值稳定性参数
  clip_range: 10.0               # 归一化裁剪范围

# 奖励归一化配置
reward_normalization:
  enabled: true                  # 启用奖励归一化
  gamma: 0.99                    # 折扣因子
  clip_range: 5.0                # 归一化值裁剪范围
  warmup_steps: 100              # 热身步数
  normalize_method: 'running_stats'  # 归一化方法
  per_env: true                  # 每个环境独立归一化

# 调试和监控
debug:
  log_level: "INFO"              # 日志级别
  monitor_frequency: 10          # 监控频率
  save_visualization: false      # 保存可视化数据