#!/usr/bin/env python3
"""
è®¾å¤‡å…¼å®¹æ€§æµ‹è¯•è„šæœ¬
ç”¨äºå¤šGPUæœåŠ¡å™¨è®­ç»ƒå‰çš„è®¾å¤‡å…¼å®¹æ€§æ£€æŸ¥
"""

import torch
import sys
import os

def check_cuda_environment():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("ğŸ” CUDAç¯å¢ƒæ£€æŸ¥:")
    if not torch.cuda.is_available():
        print("   âŒ CUDAä¸å¯ç”¨")
        return False

    print(f"   âœ… CUDAå¯ç”¨ï¼Œç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   âœ… GPUæ•°é‡: {torch.cuda.device_count()}")

    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"      è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"      æ€»å†…å­˜: {props.total_memory / 1024**3:.1f} GB")

    return True

def test_device_compatibility(target_device_id=0):
    """æµ‹è¯•è®¾å¤‡å…¼å®¹æ€§"""
    print(f"\nğŸ¯ æµ‹è¯•GPU {target_device_id}å…¼å®¹æ€§:")

    if not torch.cuda.is_available():
        print("   âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•æµ‹è¯•GPU")
        return False

    if target_device_id >= torch.cuda.device_count():
        print(f"   âŒ GPU {target_device_id} ä¸å­˜åœ¨ï¼Œåªæœ‰ {torch.cuda.device_count()} ä¸ªGPU")
        return False

    try:
        # è®¾ç½®å½“å‰è®¾å¤‡
        torch.cuda.set_device(target_device_id)
        current_device = torch.cuda.current_device()
        print(f"   âœ… è®¾ç½®å½“å‰è®¾å¤‡: GPU {current_device}")

        # æµ‹è¯•å¼ é‡åˆ›å»ºå’Œæ“ä½œ
        print("   ğŸ§ª æµ‹è¯•å¼ é‡åˆ›å»º...")
        x = torch.randn(1000, 1000, device=f'cuda:{target_device_id}')
        y = torch.randn(1000, 1000, device=f'cuda:{target_device_id}')
        z = torch.mm(x, y)
        print(f"   âœ… å¼ é‡æ“ä½œæˆåŠŸï¼Œå½¢çŠ¶: {z.shape}, è®¾å¤‡: {z.device}")

        # æµ‹è¯•å†…å­˜
        allocated = torch.cuda.memory_allocated(target_device_id)
        cached = torch.cuda.memory_reserved(target_device_id)
        print(f"   âœ… å†…å­˜ä½¿ç”¨: {allocated/1024**2:.1f} MB (å·²åˆ†é…), {cached/1024**2:.1f} MB (å·²ç¼“å­˜)")

        return True

    except Exception as e:
        print(f"   âŒ è®¾å¤‡æµ‹è¯•å¤±è´¥: {e}")
        return False

def recommend_device_config():
    """æ¨èè®¾å¤‡é…ç½®"""
    print(f"\nğŸ’¡ æ¨èè®¾å¤‡é…ç½®:")

    if not torch.cuda.is_available():
        print("   ä½¿ç”¨CPUæ¨¡å¼:")
        print("   train_isaac_fixed.py --device_id -1")
        return

    gpu_count = torch.cuda.device_count()
    print(f"   æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU:")

    for i in range(gpu_count):
        print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")

    print(f"\n   æ¨èä½¿ç”¨GPU 0ï¼ˆæœ€ç¨³å®šï¼‰:")
    print(f"   train_isaac_fixed.py --device_id 0")

    if gpu_count > 1:
        print(f"   æˆ–è€…æŒ‡å®šå…¶ä»–GPU:")
        for i in range(1, gpu_count):
            print(f"   train_isaac_fixed.py --device_id {i}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ Isaac Gym UR10e è®­ç»ƒè®¾å¤‡å…¼å®¹æ€§æ£€æŸ¥")
    print("=" * 60)

    # æ£€æŸ¥CUDAç¯å¢ƒ
    if not check_cuda_environment():
        print("\nâŒ CUDAç¯å¢ƒæ£€æŸ¥å¤±è´¥")
        sys.exit(1)

    # æµ‹è¯•é»˜è®¤è®¾å¤‡
    default_device = 0
    if not test_device_compatibility(default_device):
        print(f"\nâŒ é»˜è®¤GPU {default_device} æµ‹è¯•å¤±è´¥")
        sys.exit(1)

    # æ¨èé…ç½®
    recommend_device_config()

    print(f"\nâœ… è®¾å¤‡å…¼å®¹æ€§æ£€æŸ¥å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒ")
    print("=" * 60)

if __name__ == "__main__":
    main()